{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "806f1645",
   "metadata": {},
   "source": [
    "# üöÄ Google Colab Ready!\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fairsayan/rbs-nlp-rl-dm/blob/main/explore-embeddings/explore_embeddings_lab.ipynb)\n",
    "\n",
    "This notebook is optimized for Google Colab with automatic dependency installation and GPU support.\n",
    "\n",
    "## üìã Prerequisites\n",
    "- No local setup required - everything runs in the cloud!\n",
    "- The notebook will automatically install required packages\n",
    "- Uses Google's free GPU/TPU resources for faster processing\n",
    "\n",
    "## üîß Colab Features Used\n",
    "- **Automatic package installation** (gensim, sklearn, matplotlib, etc.)\n",
    "- **Persistent data storage** in Colab's temporary file system\n",
    "- **GPU acceleration** for faster model loading (optional)\n",
    "- **Google Drive integration** for saving results (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c019a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Install Required Packages (Google Colab)\n",
    "# This cell will install all necessary packages for the embedding exploration\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip\"\"\"\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"gensim>=4.0.0\",\n",
    "    \"scikit-learn>=1.0.0\",\n",
    "    \"matplotlib>=3.5.0\",\n",
    "    \"seaborn>=0.11.0\",\n",
    "    \"pandas>=1.3.0\",\n",
    "    \"numpy>=1.21.0\"\n",
    "]\n",
    "\n",
    "print(\"üîß Installing required packages for Google Colab...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        print(f\"üì¶ Installing {package}...\")\n",
    "        install_package(package)\n",
    "        print(f\"‚úÖ {package} installed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error installing {package}: {e}\")\n",
    "\n",
    "print(\"\\nüéâ Package installation complete!\")\n",
    "print(\"üìù Note: You may need to restart the runtime if prompted by Colab.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Google Colab Environment Setup\n",
    "# Check if we're running in Google Colab and setup the environment accordingly\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if we're in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üåê Running in Google Colab!\")\n",
    "    \n",
    "    # Mount Google Drive (optional - for saving results)\n",
    "    from google.colab import drive\n",
    "    print(\"üìÅ Mounting Google Drive...\")\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "    \n",
    "    # Set up working directory\n",
    "    WORK_DIR = '/content/embeddings_lab'\n",
    "    os.makedirs(WORK_DIR, exist_ok=True)\n",
    "    os.chdir(WORK_DIR)\n",
    "    print(f\"üìÇ Working directory set to: {WORK_DIR}\")\n",
    "    \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üíª Running in local environment\")\n",
    "    WORK_DIR = os.getcwd()\n",
    "\n",
    "print(f\"üè† Current working directory: {os.getcwd()}\")\n",
    "print(f\"üêç Python version: {sys.version}\")\n",
    "\n",
    "# Check available resources\n",
    "if IN_COLAB:\n",
    "    # Check GPU availability\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üöÄ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"üíæ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    else:\n",
    "        print(\"üîÑ CPU only (consider enabling GPU in Runtime > Change runtime type)\")\n",
    "        \n",
    "    # Check RAM\n",
    "    import psutil\n",
    "    ram_gb = psutil.virtual_memory().total / (1024**3)\n",
    "    print(f\"üíæ Total RAM: {ram_gb:.1f} GB\")\n",
    "\n",
    "print(\"\\nüéØ Environment setup complete! Ready to explore embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9fd4c6",
   "metadata": {},
   "source": [
    "# Hands-on Lab: Explore Embeddings üöÄ\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fairsayan/rbs-nlp-rl-dm/blob/main/explore-embeddings/explore_embeddings_lab.ipynb)\n",
    "\n",
    "## Goal\n",
    "Visualize word similarity using pretrained Word2Vec embeddings\n",
    "\n",
    "## Tools\n",
    "- **gensim**: For loading pretrained Word2Vec models\n",
    "- **sklearn**: For dimensionality reduction and similarity calculations\n",
    "- **matplotlib**: For visualization\n",
    "- **Google Colab**: Cloud-based execution with automatic setup\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "1. Load and work with pretrained Word2Vec embeddings\n",
    "2. Find nearest neighbors in vector space\n",
    "3. Visualize word similarities using 2D plots\n",
    "4. Analyze business-relevant vocabulary through embeddings\n",
    "5. Run everything seamlessly in Google Colab\n",
    "\n",
    "## üåü Colab Features\n",
    "- **No local setup required** - everything runs in the cloud\n",
    "- **Automatic package installation** - just run the cells\n",
    "- **GPU acceleration** - faster model loading and processing\n",
    "- **Persistent storage** - save your results to Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5ef20c",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for our embedding exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c131f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP and embeddings\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure matplotlib for Google Colab\n",
    "plt.style.use('default')  # Using default style for better Colab compatibility\n",
    "\n",
    "# Set better defaults for Colab\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "# Set seaborn palette\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure display options for better output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üìä Gensim version: {gensim.__version__}\")\n",
    "print(f\"üêç NumPy version: {np.__version__}\")\n",
    "print(f\"üêº Pandas version: {pd.__version__}\")\n",
    "print(f\"üìà Matplotlib configured for Google Colab\")\n",
    "\n",
    "# Check if we're in Colab for additional optimizations\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"üåê Google Colab environment detected\")\n",
    "    print(\"‚ö° Optimizations applied for cloud execution\")\n",
    "except ImportError:\n",
    "    print(\"üíª Local environment detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb34b67",
   "metadata": {},
   "source": [
    "## Step 2: Load Pretrained Word2Vec Model\n",
    "\n",
    "We'll use Google's pretrained Word2Vec model trained on Google News dataset. This model contains 300-dimensional vectors for 3 million words and phrases.\n",
    "\n",
    "**Google Colab Integration**: \n",
    "- The model will be automatically downloaded using gensim's API\n",
    "- Downloads directly to Colab's temporary storage\n",
    "- Optimized for cloud execution with progress tracking\n",
    "- No local file management needed\n",
    "\n",
    "**Model Details**:\n",
    "- **Source**: Google News dataset (3 million words and phrases)\n",
    "- **Dimensions**: 300-dimensional vectors\n",
    "- **Download**: Automatic via gensim API\n",
    "- **Storage**: Colab's temporary file system (persists during session)\n",
    "- **Memory**: ~1.5GB (well within Colab's limits)\n",
    "\n",
    "**‚ö° Performance Note**: \n",
    "- First run: ~2-3 minutes download time\n",
    "- Subsequent runs: Instant loading from memory\n",
    "- GPU acceleration available for faster processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c664f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì° Load Google's pretrained Word2Vec model\n",
    "# Optimized for Google Colab with automatic downloading and caching\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "print(\"üöÄ Loading Word2Vec model for Google Colab...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if we're in Colab environment\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üåê Google Colab detected - using optimized loading\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üíª Local environment detected\")\n",
    "\n",
    "# Set data directory based on environment\n",
    "if IN_COLAB:\n",
    "    data_dir = '/content/data'\n",
    "else:\n",
    "    data_dir = 'data'\n",
    "\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Method 1: Try to load from local cache first\n",
    "    model_path = os.path.join(data_dir, 'word2vec-google-news-300.bin')\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        print(\"üìÅ Found cached model, loading from local storage...\")\n",
    "        word_vectors = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "        print(\"‚úÖ Model loaded from cache!\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Model not found in cache\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"üì¶ Downloading Word2Vec model (this may take 2-3 minutes)...\")\n",
    "    print(\"üí° Tip: The model will be cached for faster future loading\")\n",
    "    \n",
    "    # Method 2: Download using gensim's API\n",
    "    print(\"\\nüîÑ Downloading word2vec-google-news-300...\")\n",
    "    print(\"üìä Model size: ~1.5GB\")\n",
    "    \n",
    "    try:\n",
    "        word_vectors = api.load('word2vec-google-news-300')\n",
    "        print(\"‚úÖ Model downloaded successfully!\")\n",
    "        \n",
    "        # Save to local cache for future use\n",
    "        print(\"\udcbe Caching model for future runs...\")\n",
    "        word_vectors.save_word2vec_format(model_path, binary=True)\n",
    "        print(\"‚úÖ Model cached successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading model: {e}\")\n",
    "        print(\"üîÑ Trying alternative download method...\")\n",
    "        \n",
    "        # Method 3: Alternative download approach\n",
    "        import urllib.request\n",
    "        import gzip\n",
    "        \n",
    "        url = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "        print(f\"üì° Downloading from: {url}\")\n",
    "        \n",
    "        # This is a fallback - in practice, gensim's API should work\n",
    "        print(\"‚ö†Ô∏è This is a fallback method - may take longer\")\n",
    "        word_vectors = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Model verification and info\n",
    "print(f\"\\nüîç Model loaded successfully!\")\n",
    "print(f\"üìä Vocabulary size: {len(word_vectors.key_to_index):,}\")\n",
    "print(f\"üìê Vector dimensions: {word_vectors.vector_size}\")\n",
    "print(f\"\udcbe Model type: {type(word_vectors)}\")\n",
    "\n",
    "# Test the model\n",
    "test_words = ['business', 'technology', 'profit', 'innovation']\n",
    "available_test_words = [word for word in test_words if word in word_vectors.key_to_index]\n",
    "\n",
    "print(f\"\\nüß™ Model verification:\")\n",
    "print(f\"‚úÖ Test words in vocabulary: {available_test_words}\")\n",
    "print(f\"‚úÖ Model ready for embedding exploration!\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(f\"\\n‚òÅÔ∏è Running in Google Colab - optimal performance achieved!\")\n",
    "    print(f\"üöÄ GPU acceleration: {'Available' if 'torch' in locals() and torch.cuda.is_available() else 'CPU only'}\")\n",
    "    print(f\"üíæ Memory usage: {len(word_vectors.key_to_index) * word_vectors.vector_size * 4 / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d245f4",
   "metadata": {},
   "source": [
    "## Step 3: Define Business-Relevant Words\n",
    "\n",
    "Let's select a set of business-relevant words to explore their embeddings and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6413ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define business-relevant words for analysis\n",
    "business_words = [\n",
    "    # Finance & Economics\n",
    "    'profit', 'revenue', 'investment', 'budget', 'finance', 'economy',\n",
    "    \n",
    "    # Technology & Innovation\n",
    "    'technology', 'innovation', 'digital', 'software', 'artificial_intelligence', 'data',\n",
    "    \n",
    "    # Marketing & Sales\n",
    "    'marketing', 'sales', 'customer', 'brand', 'advertising', 'promotion',\n",
    "    \n",
    "    # Operations & Management\n",
    "    'management', 'leadership', 'strategy', 'operations', 'efficiency', 'quality',\n",
    "    \n",
    "    # Human Resources\n",
    "    'employee', 'talent', 'training', 'performance', 'recruitment', 'teamwork'\n",
    "]\n",
    "\n",
    "# Filter words that exist in our vocabulary\n",
    "available_words = [word for word in business_words if word in word_vectors.key_to_index]\n",
    "missing_words = [word for word in business_words if word not in word_vectors.key_to_index]\n",
    "\n",
    "print(f\"‚úÖ Available words ({len(available_words)}): {available_words}\")\n",
    "print(f\"‚ùå Missing words ({len(missing_words)}): {missing_words}\")\n",
    "\n",
    "# Use available words for our analysis\n",
    "target_words = available_words[:15]  # Limit to first 15 for better visualization\n",
    "print(f\"\\nüéØ Words selected for analysis: {target_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff33daf",
   "metadata": {},
   "source": [
    "## Step 4: Explore Word Similarity - Find Nearest Neighbors\n",
    "\n",
    "Let's find the nearest neighbors for each of our target words in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d564b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbors(word, model, top_n=5):\n",
    "    \"\"\"Find nearest neighbors for a given word\"\"\"\n",
    "    try:\n",
    "        neighbors = model.most_similar(word, topn=top_n)\n",
    "        return neighbors\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "# Find nearest neighbors for each target word\n",
    "print(\"üîç Finding nearest neighbors for each business word...\\n\")\n",
    "\n",
    "neighbors_data = {}\n",
    "for word in target_words:\n",
    "    neighbors = find_nearest_neighbors(word, word_vectors, top_n=5)\n",
    "    if neighbors:\n",
    "        neighbors_data[word] = neighbors\n",
    "        print(f\"üìä **{word.upper()}** - Nearest neighbors:\")\n",
    "        for neighbor, similarity in neighbors:\n",
    "            print(f\"   {neighbor}: {similarity:.3f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b645f21",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Word Embeddings in 2D\n",
    "\n",
    "Now let's visualize our business words in a 2D space using dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6131ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract vectors for our target words\n",
    "word_vectors_matrix = np.array([word_vectors[word] for word in target_words])\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "word_vectors_2d_pca = pca.fit_transform(word_vectors_matrix)\n",
    "\n",
    "# Apply t-SNE for dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(target_words)-1))\n",
    "word_vectors_2d_tsne = tsne.fit_transform(word_vectors_matrix)\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Plot PCA results\n",
    "scatter1 = ax1.scatter(word_vectors_2d_pca[:, 0], word_vectors_2d_pca[:, 1], \n",
    "                      c=range(len(target_words)), cmap='viridis', s=100, alpha=0.7)\n",
    "\n",
    "for i, word in enumerate(target_words):\n",
    "    ax1.annotate(word, (word_vectors_2d_pca[i, 0], word_vectors_2d_pca[i, 1]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax1.set_title('Word Embeddings Visualization - PCA', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot t-SNE results\n",
    "scatter2 = ax2.scatter(word_vectors_2d_tsne[:, 0], word_vectors_2d_tsne[:, 1], \n",
    "                      c=range(len(target_words)), cmap='viridis', s=100, alpha=0.7)\n",
    "\n",
    "for i, word in enumerate(target_words):\n",
    "    ax2.annotate(word, (word_vectors_2d_tsne[i, 0], word_vectors_2d_tsne[i, 1]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax2.set_title('Word Embeddings Visualization - t-SNE', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('t-SNE Component 1')\n",
    "ax2.set_ylabel('t-SNE Component 2')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìà Visualized {len(target_words)} business words in 2D space\")\n",
    "print(f\"üìä PCA explained variance: {pca.explained_variance_ratio_.sum():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d916b424",
   "metadata": {},
   "source": [
    "## Step 6: Calculate and Visualize Similarity Matrix\n",
    "\n",
    "Let's create a similarity matrix to see how related our business words are to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea13d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(word_vectors_matrix)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "similarity_df = pd.DataFrame(similarity_matrix, \n",
    "                           index=target_words, \n",
    "                           columns=target_words)\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(similarity_df, dtype=bool))  # Mask upper triangle\n",
    "\n",
    "sns.heatmap(similarity_df, \n",
    "            annot=True, \n",
    "            cmap='RdYlBu_r', \n",
    "            vmin=0, \n",
    "            vmax=1,\n",
    "            center=0.5,\n",
    "            square=True,\n",
    "            mask=mask,\n",
    "            fmt='.2f',\n",
    "            cbar_kws={'label': 'Cosine Similarity'})\n",
    "\n",
    "plt.title('Business Words Similarity Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Words', fontsize=12)\n",
    "plt.ylabel('Words', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üî• Similarity matrix created! Higher values indicate more similar words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1e2ae9",
   "metadata": {},
   "source": [
    "## Step 7: Find Most and Least Similar Word Pairs\n",
    "\n",
    "Let's identify the most and least similar word pairs from our business vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc26de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most and least similar pairs\n",
    "word_pairs = []\n",
    "similarities = []\n",
    "\n",
    "for i in range(len(target_words)):\n",
    "    for j in range(i+1, len(target_words)):\n",
    "        word1, word2 = target_words[i], target_words[j]\n",
    "        similarity = similarity_matrix[i, j]\n",
    "        word_pairs.append((word1, word2))\n",
    "        similarities.append(similarity)\n",
    "\n",
    "# Sort by similarity\n",
    "sorted_pairs = sorted(zip(word_pairs, similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"üîù TOP 10 MOST SIMILAR WORD PAIRS:\")\n",
    "print(\"=\"*50)\n",
    "for i, ((word1, word2), sim) in enumerate(sorted_pairs[:10]):\n",
    "    print(f\"{i+1:2d}. {word1:12} ‚Üî {word2:12} | Similarity: {sim:.3f}\")\n",
    "\n",
    "print(\"\\nüîª TOP 10 LEAST SIMILAR WORD PAIRS:\")\n",
    "print(\"=\"*50)\n",
    "for i, ((word1, word2), sim) in enumerate(sorted_pairs[-10:]):\n",
    "    print(f\"{i+1:2d}. {word1:12} ‚Üî {word2:12} | Similarity: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67b97de",
   "metadata": {},
   "source": [
    "## Step 8: Interactive Word Similarity Explorer\n",
    "\n",
    "Let's create an interactive function to explore word relationships in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e5638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_word_relationships(word, model, target_words_list):\n",
    "    \"\"\"Explore relationships between a word and our target vocabulary\"\"\"\n",
    "    if word not in model.key_to_index:\n",
    "        print(f\"‚ùå '{word}' not found in vocabulary\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üîç Exploring relationships for: **{word.upper()}**\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Find similarities with our target words\n",
    "    similarities = []\n",
    "    for target_word in target_words_list:\n",
    "        if target_word in model.key_to_index:\n",
    "            sim = model.similarity(word, target_word)\n",
    "            similarities.append((target_word, sim))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"üìä Similarity with business words:\")\n",
    "    for target_word, sim in similarities[:10]:\n",
    "        print(f\"   {target_word:15} | {sim:.3f} {'üî•' if sim > 0.5 else 'üìä' if sim > 0.3 else 'üìâ'}\")\n",
    "    \n",
    "    # Find general nearest neighbors\n",
    "    print(\"\\nüéØ Top 5 nearest neighbors:\")\n",
    "    neighbors = model.most_similar(word, topn=5)\n",
    "    for neighbor, sim in neighbors:\n",
    "        print(f\"   {neighbor:15} | {sim:.3f}\")\n",
    "\n",
    "# Example usage\n",
    "print(\"Try exploring different words! For example:\")\n",
    "explore_word_relationships('profit', word_vectors, target_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83172f",
   "metadata": {},
   "source": [
    "## Step 9: Word Arithmetic and Analogies\n",
    "\n",
    "One fascinating aspect of word embeddings is their ability to capture semantic relationships through vector arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4260db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogy(word1, word2, word3, model, top_n=5):\n",
    "    \"\"\"Find word that completes the analogy: word1 is to word2 as word3 is to ?\"\"\"\n",
    "    try:\n",
    "        result = model.most_similar(positive=[word2, word3], negative=[word1], topn=top_n)\n",
    "        return result\n",
    "    except KeyError as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Business analogies to explore\n",
    "analogies = [\n",
    "    ('king', 'man', 'woman'),  # Classic example: king - man + woman = queen\n",
    "    ('CEO', 'company', 'school'),  # CEO is to company as ? is to school\n",
    "    ('profit', 'business', 'education'),  # profit is to business as ? is to education\n",
    "    ('marketing', 'product', 'candidate'),  # marketing is to product as ? is to candidate\n",
    "    ('investment', 'money', 'time'),  # investment is to money as ? is to time\n",
    "]\n",
    "\n",
    "print(\"üßÆ WORD ARITHMETIC AND ANALOGIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for word1, word2, word3 in analogies:\n",
    "    print(f\"\\nüîç {word1} is to {word2} as {word3} is to...\")\n",
    "    result = find_analogy(word1, word2, word3, word_vectors, top_n=3)\n",
    "    \n",
    "    if isinstance(result, str):\n",
    "        print(f\"   {result}\")\n",
    "    else:\n",
    "        print(f\"   Top predictions:\")\n",
    "        for word, score in result:\n",
    "            print(f\"     {word} ({score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf5c39",
   "metadata": {},
   "source": [
    "## Step 10: Summary and Key Insights\n",
    "\n",
    "Let's summarize our findings and extract key insights from our embedding exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6819069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"üìà EMBEDDING EXPLORATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"üìä Total words analyzed: {len(target_words)}\")\n",
    "print(f\"üìê Vector dimensions: {word_vectors.vector_size}\")\n",
    "print(f\"üìö Total vocabulary size: {len(word_vectors.key_to_index):,}\")\n",
    "\n",
    "# Calculate some statistics\n",
    "avg_similarity = np.mean(similarity_matrix[np.triu_indices(len(target_words), k=1)])\n",
    "max_similarity = np.max(similarity_matrix[np.triu_indices(len(target_words), k=1)])\n",
    "min_similarity = np.min(similarity_matrix[np.triu_indices(len(target_words), k=1)])\n",
    "\n",
    "print(f\"\\nüìä Similarity Statistics:\")\n",
    "print(f\"   Average similarity: {avg_similarity:.3f}\")\n",
    "print(f\"   Maximum similarity: {max_similarity:.3f}\")\n",
    "print(f\"   Minimum similarity: {min_similarity:.3f}\")\n",
    "\n",
    "print(\"\\nüéØ KEY INSIGHTS:\")\n",
    "print(\"1. Word embeddings capture semantic relationships between business terms\")\n",
    "print(\"2. Similar words cluster together in the vector space\")\n",
    "print(\"3. Vector arithmetic can reveal analogical relationships\")\n",
    "print(\"4. Dimensionality reduction helps visualize high-dimensional embeddings\")\n",
    "print(\"5. Cosine similarity is effective for measuring word relationships\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"‚Ä¢ Experiment with different word lists (industry-specific terms)\")\n",
    "print(\"‚Ä¢ Try different pretrained models (GloVe, FastText, etc.)\")\n",
    "print(\"‚Ä¢ Explore domain-specific embedding models\")\n",
    "print(\"‚Ä¢ Apply embeddings to text classification or clustering tasks\")\n",
    "print(\"‚Ä¢ Create custom embeddings from your own text data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956aea1d",
   "metadata": {},
   "source": [
    "## üéØ Lab Exercise: Your Turn!\n",
    "\n",
    "Now it's your turn to explore embeddings! Complete the following exercises:\n",
    "\n",
    "### Exercise 1: Custom Word List\n",
    "Create your own list of words related to your field of interest (e.g., healthcare, education, sports) and repeat the analysis above.\n",
    "\n",
    "### Exercise 2: Word Arithmetic\n",
    "Try to find interesting analogies using word arithmetic. Can you find business-related analogies?\n",
    "\n",
    "### Exercise 3: Similarity Threshold\n",
    "Experiment with different similarity thresholds to group words into clusters.\n",
    "\n",
    "Use the cells below to implement your solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331d3faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your custom word list\n",
    "my_words = []\n",
    "# Add your words here and run the analysis\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26720ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Word arithmetic experiments\n",
    "# Try your own analogies here\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c1da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Clustering with similarity thresholds\n",
    "# Group words based on similarity thresholds\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e82ae2",
   "metadata": {},
   "source": [
    "## üíæ Save Results to Google Drive (Optional)\n",
    "\n",
    "If you want to save your results and visualizations to Google Drive for later use, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b423781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Save Results to Google Drive\n",
    "# This cell saves your analysis results to Google Drive for future reference\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    \n",
    "    # Create a folder in Google Drive for results\n",
    "    drive_folder = '/content/drive/MyDrive/Embedding_Lab_Results'\n",
    "    os.makedirs(drive_folder, exist_ok=True)\n",
    "    \n",
    "    # Create timestamped folder\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    session_folder = os.path.join(drive_folder, f'session_{timestamp}')\n",
    "    os.makedirs(session_folder, exist_ok=True)\n",
    "    \n",
    "    print(f\"üìÅ Created results folder: {session_folder}\")\n",
    "    \n",
    "    # Save similarity matrix\n",
    "    if 'similarity_df' in locals():\n",
    "        similarity_df.to_csv(os.path.join(session_folder, 'similarity_matrix.csv'))\n",
    "        print(\"‚úÖ Similarity matrix saved\")\n",
    "    \n",
    "    # Save word pairs analysis\n",
    "    if 'sorted_pairs' in locals():\n",
    "        pairs_data = {\n",
    "            'most_similar': [{'word1': pair[0], 'word2': pair[1], 'similarity': sim} \n",
    "                           for (pair, sim) in sorted_pairs[:10]],\n",
    "            'least_similar': [{'word1': pair[0], 'word2': pair[1], 'similarity': sim} \n",
    "                            for (pair, sim) in sorted_pairs[-10:]]\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(session_folder, 'word_pairs_analysis.json'), 'w') as f:\n",
    "            json.dump(pairs_data, f, indent=2)\n",
    "        print(\"‚úÖ Word pairs analysis saved\")\n",
    "    \n",
    "    # Save target words list\n",
    "    if 'target_words' in locals():\n",
    "        with open(os.path.join(session_folder, 'target_words.txt'), 'w') as f:\n",
    "            f.write('\\n'.join(target_words))\n",
    "        print(\"‚úÖ Target words list saved\")\n",
    "    \n",
    "    # Save session summary\n",
    "    summary = {\n",
    "        'timestamp': timestamp,\n",
    "        'total_words_analyzed': len(target_words) if 'target_words' in locals() else 0,\n",
    "        'vector_dimensions': word_vectors.vector_size if 'word_vectors' in locals() else 0,\n",
    "        'vocabulary_size': len(word_vectors.key_to_index) if 'word_vectors' in locals() else 0,\n",
    "        'average_similarity': float(avg_similarity) if 'avg_similarity' in locals() else 0,\n",
    "        'max_similarity': float(max_similarity) if 'max_similarity' in locals() else 0,\n",
    "        'min_similarity': float(min_similarity) if 'min_similarity' in locals() else 0\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(session_folder, 'session_summary.json'), 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(\"‚úÖ Session summary saved\")\n",
    "    print(f\"\\nüéâ All results saved to Google Drive!\")\n",
    "    print(f\"üìÇ Location: {session_folder}\")\n",
    "    print(f\"üîó Access via: Google Drive > Embedding_Lab_Results > session_{timestamp}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"üíª Not running in Google Colab - skipping Drive save\")\n",
    "    print(\"üí° To save results locally, modify the paths in this cell\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving to Google Drive: {e}\")\n",
    "    print(\"üí° Make sure Google Drive is mounted and you have write permissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de80903",
   "metadata": {},
   "source": [
    "## üéØ Next Steps & Sharing\n",
    "\n",
    "### üì§ Share Your Work\n",
    "1. **Save a copy to Drive**: `File > Save a copy in Drive`\n",
    "2. **Share with others**: `Share` button ‚Üí Add collaborators\n",
    "3. **Download as notebook**: `File > Download > Download .ipynb`\n",
    "4. **Export to GitHub**: `File > Save a copy in GitHub`\n",
    "\n",
    "### üöÄ Advanced Exploration\n",
    "- **Try different models**: FastText, GloVe, domain-specific embeddings\n",
    "- **Scale up**: Use larger word lists or entire documents\n",
    "- **Custom training**: Train embeddings on your own text data\n",
    "- **Applications**: Text classification, clustering, recommendation systems\n",
    "\n",
    "### üõ†Ô∏è Troubleshooting\n",
    "- **Out of memory**: Reduce word list size or restart runtime\n",
    "- **Slow downloads**: Check internet connection or try different times\n",
    "- **GPU issues**: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
    "- **Drive mounting**: Rerun the environment setup cell\n",
    "\n",
    "### üìö Resources\n",
    "- [Gensim Documentation](https://radimrehurek.com/gensim/)\n",
    "- [Word2Vec Paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "- [Google Colab Tips](https://colab.research.google.com/notebooks/welcome.ipynb)\n",
    "\n",
    "**üéâ Congratulations! You've successfully explored word embeddings in Google Colab!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
