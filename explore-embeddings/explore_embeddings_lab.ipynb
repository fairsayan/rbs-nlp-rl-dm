{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9fd4c6",
   "metadata": {},
   "source": [
    "# Hands-on Lab: Explore Embeddings\n",
    "\n",
    "## Goal\n",
    "Visualize word similarity using pretrained Word2Vec embeddings\n",
    "\n",
    "## Tools\n",
    "- **gensim**: For loading pretrained Word2Vec models\n",
    "- **sklearn**: For dimensionality reduction and similarity calculations\n",
    "- **matplotlib**: For visualization\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "1. Load and work with pretrained Word2Vec embeddings\n",
    "2. Find nearest neighbors in vector space\n",
    "3. Visualize word similarities using 2D plots\n",
    "4. Analyze business-relevant vocabulary through embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5ef20c",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for our embedding exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c131f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP and embeddings\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Gensim version: {gensim.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb34b67",
   "metadata": {},
   "source": [
    "## Step 2: Load Pretrained Word2Vec Model\n",
    "\n",
    "We'll use Google's pretrained Word2Vec model trained on Google News dataset. This model contains 300-dimensional vectors for 3 million words and phrases.\n",
    "\n",
    "**Local Data Management**: \n",
    "- The model will be automatically downloaded to a local `data/` directory\n",
    "- Subsequent runs will load the model directly from the local directory\n",
    "- This ensures faster loading and better project organization\n",
    "\n",
    "**Model Details**:\n",
    "- **Source**: Google News dataset (3 million words and phrases)\n",
    "- **Dimensions**: 300-dimensional vectors\n",
    "- **Local Path**: `./data/word2vec-google-news-300.bin`\n",
    "- **Fallback**: Downloads automatically if not found locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c664f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load Google's pretrained Word2Vec model\n",
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "data_dir = 'data'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Try to load from local data directory first\n",
    "    model_path = os.path.join(data_dir, 'word2vec-google-news-300.bin')\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "    print(\"‚úÖ Successfully loaded Word2Vec model from local data directory\")\n",
    "    print(f\"üìÅ Loaded from: {model_path}\")\n",
    "    print(f\"Vocabulary size: {len(word_vectors.key_to_index):,}\")\n",
    "    print(f\"Vector dimensions: {word_vectors.vector_size}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        # Try the original Google News model filename\n",
    "        model_path = os.path.join(data_dir, 'GoogleNews-vectors-negative300.bin')\n",
    "        word_vectors = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "        print(\"‚úÖ Successfully loaded Google News Word2Vec model from local data directory\")\n",
    "        print(f\"üìÅ Loaded from: {model_path}\")\n",
    "        print(f\"Vocabulary size: {len(word_vectors.key_to_index):,}\")\n",
    "        print(f\"Vector dimensions: {word_vectors.vector_size}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå No model found in data directory. Downloading...\")\n",
    "        print(\"This will download the model to the local data directory.\")\n",
    "        \n",
    "        # Use gensim's API to download the model\n",
    "        import gensim.downloader as api\n",
    "        \n",
    "        # Download word2vec-google-news-300 model\n",
    "        print(\"Downloading word2vec-google-news-300 model...\")\n",
    "        word_vectors = api.load('word2vec-google-news-300')\n",
    "        \n",
    "        # Save the model to our data directory for future use\n",
    "        model_save_path = os.path.join(data_dir, 'word2vec-google-news-300.bin')\n",
    "        word_vectors.save_word2vec_format(model_save_path, binary=True)\n",
    "        print(f\"‚úÖ Model downloaded and saved to: {model_save_path}\")\n",
    "        \n",
    "        print(\"‚úÖ Successfully loaded Word2Vec model from gensim API\")\n",
    "        print(f\"Vocabulary size: {len(word_vectors.key_to_index):,}\")\n",
    "        print(f\"Vector dimensions: {word_vectors.vector_size}\")\n",
    "        print(f\"üìÅ Model cached in: ./{data_dir}/\")\n",
    "\n",
    "# Verify the model is working\n",
    "print(f\"\\nüîç Model verification:\")\n",
    "print(f\"   Variable 'word_vectors' is ready for use\")\n",
    "print(f\"   Model type: {type(word_vectors)}\")\n",
    "print(f\"   Sample word 'business' in vocabulary: {'business' in word_vectors.key_to_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d245f4",
   "metadata": {},
   "source": [
    "## Step 3: Define Business-Relevant Words\n",
    "\n",
    "Let's select a set of business-relevant words to explore their embeddings and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6413ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define business-relevant words for analysis\n",
    "business_words = [\n",
    "    # Finance & Economics\n",
    "    'profit', 'revenue', 'investment', 'budget', 'finance', 'economy',\n",
    "    \n",
    "    # Technology & Innovation\n",
    "    'technology', 'innovation', 'digital', 'software', 'artificial_intelligence', 'data',\n",
    "    \n",
    "    # Marketing & Sales\n",
    "    'marketing', 'sales', 'customer', 'brand', 'advertising', 'promotion',\n",
    "    \n",
    "    # Operations & Management\n",
    "    'management', 'leadership', 'strategy', 'operations', 'efficiency', 'quality',\n",
    "    \n",
    "    # Human Resources\n",
    "    'employee', 'talent', 'training', 'performance', 'recruitment', 'teamwork'\n",
    "]\n",
    "\n",
    "# Filter words that exist in our vocabulary\n",
    "available_words = [word for word in business_words if word in word_vectors.key_to_index]\n",
    "missing_words = [word for word in business_words if word not in word_vectors.key_to_index]\n",
    "\n",
    "print(f\"‚úÖ Available words ({len(available_words)}): {available_words}\")\n",
    "print(f\"‚ùå Missing words ({len(missing_words)}): {missing_words}\")\n",
    "\n",
    "# Use available words for our analysis\n",
    "target_words = available_words[:15]  # Limit to first 15 for better visualization\n",
    "print(f\"\\nüéØ Words selected for analysis: {target_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff33daf",
   "metadata": {},
   "source": [
    "## Step 4: Explore Word Similarity - Find Nearest Neighbors\n",
    "\n",
    "Let's find the nearest neighbors for each of our target words in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d564b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbors(word, model, top_n=5):\n",
    "    \"\"\"Find nearest neighbors for a given word\"\"\"\n",
    "    try:\n",
    "        neighbors = model.most_similar(word, topn=top_n)\n",
    "        return neighbors\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "# Find nearest neighbors for each target word\n",
    "print(\"üîç Finding nearest neighbors for each business word...\\n\")\n",
    "\n",
    "neighbors_data = {}\n",
    "for word in target_words:\n",
    "    neighbors = find_nearest_neighbors(word, word_vectors, top_n=5)\n",
    "    if neighbors:\n",
    "        neighbors_data[word] = neighbors\n",
    "        print(f\"üìä **{word.upper()}** - Nearest neighbors:\")\n",
    "        for neighbor, similarity in neighbors:\n",
    "            print(f\"   {neighbor}: {similarity:.3f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b645f21",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Word Embeddings in 2D\n",
    "\n",
    "Now let's visualize our business words in a 2D space using dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6131ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract vectors for our target words\n",
    "word_vectors_matrix = np.array([word_vectors[word] for word in target_words])\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "word_vectors_2d_pca = pca.fit_transform(word_vectors_matrix)\n",
    "\n",
    "# Apply t-SNE for dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(target_words)-1))\n",
    "word_vectors_2d_tsne = tsne.fit_transform(word_vectors_matrix)\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Plot PCA results\n",
    "scatter1 = ax1.scatter(word_vectors_2d_pca[:, 0], word_vectors_2d_pca[:, 1], \n",
    "                      c=range(len(target_words)), cmap='viridis', s=100, alpha=0.7)\n",
    "\n",
    "for i, word in enumerate(target_words):\n",
    "    ax1.annotate(word, (word_vectors_2d_pca[i, 0], word_vectors_2d_pca[i, 1]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax1.set_title('Word Embeddings Visualization - PCA', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot t-SNE results\n",
    "scatter2 = ax2.scatter(word_vectors_2d_tsne[:, 0], word_vectors_2d_tsne[:, 1], \n",
    "                      c=range(len(target_words)), cmap='viridis', s=100, alpha=0.7)\n",
    "\n",
    "for i, word in enumerate(target_words):\n",
    "    ax2.annotate(word, (word_vectors_2d_tsne[i, 0], word_vectors_2d_tsne[i, 1]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax2.set_title('Word Embeddings Visualization - t-SNE', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('t-SNE Component 1')\n",
    "ax2.set_ylabel('t-SNE Component 2')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìà Visualized {len(target_words)} business words in 2D space\")\n",
    "print(f\"üìä PCA explained variance: {pca.explained_variance_ratio_.sum():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d916b424",
   "metadata": {},
   "source": [
    "## Step 6: Calculate and Visualize Similarity Matrix\n",
    "\n",
    "Let's create a similarity matrix to see how related our business words are to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea13d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(word_vectors_matrix)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "similarity_df = pd.DataFrame(similarity_matrix, \n",
    "                           index=target_words, \n",
    "                           columns=target_words)\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(similarity_df, dtype=bool))  # Mask upper triangle\n",
    "\n",
    "sns.heatmap(similarity_df, \n",
    "            annot=True, \n",
    "            cmap='RdYlBu_r', \n",
    "            vmin=0, \n",
    "            vmax=1,\n",
    "            center=0.5,\n",
    "            square=True,\n",
    "            mask=mask,\n",
    "            fmt='.2f',\n",
    "            cbar_kws={'label': 'Cosine Similarity'})\n",
    "\n",
    "plt.title('Business Words Similarity Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Words', fontsize=12)\n",
    "plt.ylabel('Words', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üî• Similarity matrix created! Higher values indicate more similar words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1e2ae9",
   "metadata": {},
   "source": [
    "## Step 7: Find Most and Least Similar Word Pairs\n",
    "\n",
    "Let's identify the most and least similar word pairs from our business vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc26de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most and least similar pairs\n",
    "word_pairs = []\n",
    "similarities = []\n",
    "\n",
    "for i in range(len(target_words)):\n",
    "    for j in range(i+1, len(target_words)):\n",
    "        word1, word2 = target_words[i], target_words[j]\n",
    "        similarity = similarity_matrix[i, j]\n",
    "        word_pairs.append((word1, word2))\n",
    "        similarities.append(similarity)\n",
    "\n",
    "# Sort by similarity\n",
    "sorted_pairs = sorted(zip(word_pairs, similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"üîù TOP 10 MOST SIMILAR WORD PAIRS:\")\n",
    "print(\"=\"*50)\n",
    "for i, ((word1, word2), sim) in enumerate(sorted_pairs[:10]):\n",
    "    print(f\"{i+1:2d}. {word1:12} ‚Üî {word2:12} | Similarity: {sim:.3f}\")\n",
    "\n",
    "print(\"\\nüîª TOP 10 LEAST SIMILAR WORD PAIRS:\")\n",
    "print(\"=\"*50)\n",
    "for i, ((word1, word2), sim) in enumerate(sorted_pairs[-10:]):\n",
    "    print(f\"{i+1:2d}. {word1:12} ‚Üî {word2:12} | Similarity: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67b97de",
   "metadata": {},
   "source": [
    "## Step 8: Interactive Word Similarity Explorer\n",
    "\n",
    "Let's create an interactive function to explore word relationships in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e5638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_word_relationships(word, model, target_words_list):\n",
    "    \"\"\"Explore relationships between a word and our target vocabulary\"\"\"\n",
    "    if word not in model.key_to_index:\n",
    "        print(f\"‚ùå '{word}' not found in vocabulary\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üîç Exploring relationships for: **{word.upper()}**\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Find similarities with our target words\n",
    "    similarities = []\n",
    "    for target_word in target_words_list:\n",
    "        if target_word in model.key_to_index:\n",
    "            sim = model.similarity(word, target_word)\n",
    "            similarities.append((target_word, sim))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"üìä Similarity with business words:\")\n",
    "    for target_word, sim in similarities[:10]:\n",
    "        print(f\"   {target_word:15} | {sim:.3f} {'üî•' if sim > 0.5 else 'üìä' if sim > 0.3 else 'üìâ'}\")\n",
    "    \n",
    "    # Find general nearest neighbors\n",
    "    print(\"\\nüéØ Top 5 nearest neighbors:\")\n",
    "    neighbors = model.most_similar(word, topn=5)\n",
    "    for neighbor, sim in neighbors:\n",
    "        print(f\"   {neighbor:15} | {sim:.3f}\")\n",
    "\n",
    "# Example usage\n",
    "print(\"Try exploring different words! For example:\")\n",
    "explore_word_relationships('profit', word_vectors, target_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83172f",
   "metadata": {},
   "source": [
    "## Step 9: Word Arithmetic and Analogies\n",
    "\n",
    "One fascinating aspect of word embeddings is their ability to capture semantic relationships through vector arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4260db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogy(word1, word2, word3, model, top_n=5):\n",
    "    \"\"\"Find word that completes the analogy: word1 is to word2 as word3 is to ?\"\"\"\n",
    "    try:\n",
    "        result = model.most_similar(positive=[word2, word3], negative=[word1], topn=top_n)\n",
    "        return result\n",
    "    except KeyError as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Business analogies to explore\n",
    "analogies = [\n",
    "    ('king', 'man', 'woman'),  # Classic example: king - man + woman = queen\n",
    "    ('CEO', 'company', 'school'),  # CEO is to company as ? is to school\n",
    "    ('profit', 'business', 'education'),  # profit is to business as ? is to education\n",
    "    ('marketing', 'product', 'candidate'),  # marketing is to product as ? is to candidate\n",
    "    ('investment', 'money', 'time'),  # investment is to money as ? is to time\n",
    "]\n",
    "\n",
    "print(\"üßÆ WORD ARITHMETIC AND ANALOGIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for word1, word2, word3 in analogies:\n",
    "    print(f\"\\nüîç {word1} is to {word2} as {word3} is to...\")\n",
    "    result = find_analogy(word1, word2, word3, word_vectors, top_n=3)\n",
    "    \n",
    "    if isinstance(result, str):\n",
    "        print(f\"   {result}\")\n",
    "    else:\n",
    "        print(f\"   Top predictions:\")\n",
    "        for word, score in result:\n",
    "            print(f\"     {word} ({score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf5c39",
   "metadata": {},
   "source": [
    "## Step 10: Summary and Key Insights\n",
    "\n",
    "Let's summarize our findings and extract key insights from our embedding exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6819069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"üìà EMBEDDING EXPLORATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"üìä Total words analyzed: {len(target_words)}\")\n",
    "print(f\"üìê Vector dimensions: {word_vectors.vector_size}\")\n",
    "print(f\"üìö Total vocabulary size: {len(word_vectors.key_to_index):,}\")\n",
    "\n",
    "# Calculate some statistics\n",
    "avg_similarity = np.mean(similarity_matrix[np.triu_indices(len(target_words), k=1)])\n",
    "max_similarity = np.max(similarity_matrix[np.triu_indices(len(target_words), k=1)])\n",
    "min_similarity = np.min(similarity_matrix[np.triu_indices(len(target_words), k=1)])\n",
    "\n",
    "print(f\"\\nüìä Similarity Statistics:\")\n",
    "print(f\"   Average similarity: {avg_similarity:.3f}\")\n",
    "print(f\"   Maximum similarity: {max_similarity:.3f}\")\n",
    "print(f\"   Minimum similarity: {min_similarity:.3f}\")\n",
    "\n",
    "print(\"\\nüéØ KEY INSIGHTS:\")\n",
    "print(\"1. Word embeddings capture semantic relationships between business terms\")\n",
    "print(\"2. Similar words cluster together in the vector space\")\n",
    "print(\"3. Vector arithmetic can reveal analogical relationships\")\n",
    "print(\"4. Dimensionality reduction helps visualize high-dimensional embeddings\")\n",
    "print(\"5. Cosine similarity is effective for measuring word relationships\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"‚Ä¢ Experiment with different word lists (industry-specific terms)\")\n",
    "print(\"‚Ä¢ Try different pretrained models (GloVe, FastText, etc.)\")\n",
    "print(\"‚Ä¢ Explore domain-specific embedding models\")\n",
    "print(\"‚Ä¢ Apply embeddings to text classification or clustering tasks\")\n",
    "print(\"‚Ä¢ Create custom embeddings from your own text data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956aea1d",
   "metadata": {},
   "source": [
    "## üéØ Lab Exercise: Your Turn!\n",
    "\n",
    "Now it's your turn to explore embeddings! Complete the following exercises:\n",
    "\n",
    "### Exercise 1: Custom Word List\n",
    "Create your own list of words related to your field of interest (e.g., healthcare, education, sports) and repeat the analysis above.\n",
    "\n",
    "### Exercise 2: Word Arithmetic\n",
    "Try to find interesting analogies using word arithmetic. Can you find business-related analogies?\n",
    "\n",
    "### Exercise 3: Similarity Threshold\n",
    "Experiment with different similarity thresholds to group words into clusters.\n",
    "\n",
    "Use the cells below to implement your solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331d3faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your custom word list\n",
    "my_words = []\n",
    "# Add your words here and run the analysis\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26720ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Word arithmetic experiments\n",
    "# Try your own analogies here\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c1da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Clustering with similarity thresholds\n",
    "# Group words based on similarity thresholds\n",
    "\n",
    "# Your code here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
