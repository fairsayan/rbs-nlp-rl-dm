{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f023964",
   "metadata": {},
   "source": [
    "# Amazon Reviews Analysis Laboratory\n",
    "\n",
    "## Introduction\n",
    "Welcome to the Amazon Reviews Analysis Laboratory! In this session, we will explore natural language processing techniques to clean and preprocess customer reviews from Amazon.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this laboratory, you will be able to:\n",
    "1. Load and explore text datasets using pandas\n",
    "2. Apply text preprocessing techniques including tokenization\n",
    "3. Remove stopwords from text data\n",
    "4. Visualize text data using word clouds and frequency plots\n",
    "5. Prepare text data for vectorization and machine learning\n",
    "\n",
    "### Dataset\n",
    "We'll work with Amazon product reviews, which contain valuable customer feedback that can be analyzed for sentiment, topics, and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe57926",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2292dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "\n",
    "# Configure NLTK data path to use local directory\n",
    "nltk_data_dir = os.path.join(os.getcwd(), 'nltk_data')\n",
    "os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "nltk.data.path.insert(0, nltk_data_dir)\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine learning preparation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# Display settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"üìÅ NLTK data directory configured: {nltk_data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6504daa",
   "metadata": {},
   "source": [
    "## Step 2: Download NLTK Data\n",
    "\n",
    "Download necessary NLTK datasets for tokenization and stopword removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825e694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data to local directory\n",
    "# (Directory and path already configured in Step 1)\n",
    "\n",
    "print(f\"Downloading NLTK data to: {nltk_data_dir}\")\n",
    "\n",
    "# Download NLTK data to the local directory\n",
    "# Note: punkt_tab is the new format required by recent NLTK versions\n",
    "nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "nltk.download('punkt_tab', download_dir=nltk_data_dir)\n",
    "nltk.download('stopwords', download_dir=nltk_data_dir)\n",
    "nltk.download('wordnet', download_dir=nltk_data_dir)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir=nltk_data_dir)\n",
    "nltk.download('omw-1.4', download_dir=nltk_data_dir)\n",
    "\n",
    "print(\"‚úÖ NLTK data downloaded successfully to local directory!\")\n",
    "print(f\"üìÅ Data location: {nltk_data_dir}\")\n",
    "\n",
    "# Verify NLTK can find the data\n",
    "try:\n",
    "    # Test if NLTK can access the downloaded data\n",
    "    import nltk.data\n",
    "    # Try both punkt and punkt_tab\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt_tab')\n",
    "        print(\"‚úÖ NLTK punkt_tab data found successfully!\")\n",
    "    except LookupError:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        print(\"‚úÖ NLTK punkt data found successfully!\")\n",
    "    \n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    print(\"‚úÖ NLTK stopwords data found successfully!\")\n",
    "    print(\"‚úÖ NLTK data path verification successful!\")\n",
    "except LookupError as e:\n",
    "    print(f\"‚ö†Ô∏è NLTK data not found: {e}\")\n",
    "    print(\"Data will be downloaded automatically on first use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f32b41",
   "metadata": {},
   "source": [
    "## Step 3: Create Sample Dataset\n",
    "\n",
    "Since we don't have a real dataset available, let's create a sample Amazon reviews dataset for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3039bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample Amazon reviews dataset\n",
    "sample_reviews = [\n",
    "    \"This product is absolutely amazing! I love it so much. Great quality and fast shipping.\",\n",
    "    \"Poor quality product. Disappointed with the purchase. Would not recommend to others.\",\n",
    "    \"The item arrived on time and works perfectly. Excellent customer service and support.\",\n",
    "    \"Terrible experience. The product broke after just one day. Waste of money!\",\n",
    "    \"Good value for money. The product meets my expectations. Will buy again.\",\n",
    "    \"Outstanding product! Exceeded all my expectations. Highly recommended for everyone.\",\n",
    "    \"Average product. Nothing special but does the job. Delivery was slow though.\",\n",
    "    \"Fantastic quality and design. Love the color and functionality. Perfect for my needs.\",\n",
    "    \"Not worth the price. Found better alternatives elsewhere. Customer service was unhelpful.\",\n",
    "    \"Amazing product with great features. Easy to use and very durable. Five stars!\",\n",
    "    \"The product description was misleading. Different from what I expected. Requesting refund.\",\n",
    "    \"Superb quality and excellent packaging. Arrived earlier than expected. Very satisfied.\",\n",
    "    \"Mediocre product at best. Works but could be better. Considering other options.\",\n",
    "    \"Love this product! It's exactly what I was looking for. Great design and functionality.\",\n",
    "    \"Horrible experience. Product damaged during shipping. Poor customer support response.\",\n",
    "    \"Excellent product with amazing features. Worth every penny. Highly recommended!\",\n",
    "    \"The product is okay but overpriced. Similar products available at lower prices.\",\n",
    "    \"Perfect product for my needs. Great quality and fast delivery. Will order again.\",\n",
    "    \"Disappointed with the quality. Not as described in the listing. Returning soon.\",\n",
    "    \"Outstanding customer service and product quality. Exceeded my expectations completely.\"\n",
    "]\n",
    "\n",
    "# Create ratings (1-5 stars)\n",
    "ratings = [5, 1, 4, 1, 3, 5, 3, 4, 2, 5, 2, 5, 3, 4, 1, 5, 2, 4, 2, 5]\n",
    "\n",
    "# Create product categories\n",
    "categories = ['Electronics', 'Books', 'Home & Garden', 'Sports', 'Electronics', \n",
    "             'Books', 'Home & Garden', 'Electronics', 'Sports', 'Books',\n",
    "             'Electronics', 'Home & Garden', 'Sports', 'Electronics', 'Books',\n",
    "             'Home & Garden', 'Sports', 'Electronics', 'Books', 'Home & Garden']\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'review_text': sample_reviews,\n",
    "    'rating': ratings,\n",
    "    'category': categories\n",
    "})\n",
    "\n",
    "# Add review ID\n",
    "df['review_id'] = range(1, len(df) + 1)\n",
    "\n",
    "print(f\"Dataset created with {len(df)} reviews\")\n",
    "print(\"\\nFirst 5 reviews:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e67cc5",
   "metadata": {},
   "source": [
    "## Step 4: Load and Explore Dataset\n",
    "\n",
    "Let's explore our dataset to understand its structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c091269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset basic information\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nColumn Information:\")\n",
    "print(df.info())\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37251f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Display sample reviews\n",
    "print(\"\\nSample Reviews:\")\n",
    "for i in range(3):\n",
    "    print(f\"Review {i+1} (Rating: {df.iloc[i]['rating']})\")\n",
    "    print(f\"Text: {df.iloc[i]['review_text']}\")\n",
    "    print(f\"Category: {df.iloc[i]['category']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea82a77",
   "metadata": {},
   "source": [
    "## Step 5: Basic Text Analysis\n",
    "\n",
    "Let's perform some basic analysis on our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text length statistics\n",
    "df['text_length'] = df['review_text'].apply(len)\n",
    "df['word_count'] = df['review_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"Text Length Statistics:\")\n",
    "print(f\"Average text length: {df['text_length'].mean():.2f} characters\")\n",
    "print(f\"Average word count: {df['word_count'].mean():.2f} words\")\n",
    "print(f\"Min text length: {df['text_length'].min()} characters\")\n",
    "print(f\"Max text length: {df['text_length'].max()} characters\")\n",
    "\n",
    "# Visualize text length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Text length distribution\n",
    "axes[0].hist(df['text_length'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Text Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Review Text Length')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Word count distribution\n",
    "axes[1].hist(df['word_count'], bins=10, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Review Word Count')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f15dba8",
   "metadata": {},
   "source": [
    "## Step 6: Text Preprocessing Functions\n",
    "\n",
    "Let's create functions for text preprocessing including cleaning, tokenization, and stopword removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b4041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing special characters, converting to lowercase, \n",
    "    and removing extra whitespace.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize text into individual words.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Remove English stopwords from token list.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Lemmatize tokens to their base form.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline.\n",
    "    \"\"\"\n",
    "    # Clean text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenize_text(cleaned_text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    # Lemmatize\n",
    "    tokens = lemmatize_tokens(tokens)\n",
    "    \n",
    "    # Remove empty tokens\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "print(\"Text preprocessing functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34198114",
   "metadata": {},
   "source": [
    "## Step 7: Apply Text Preprocessing\n",
    "\n",
    "Now let's apply our preprocessing functions to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811babe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to all reviews\n",
    "print(\"Applying text preprocessing...\")\n",
    "\n",
    "# Clean text\n",
    "df['cleaned_text'] = df['review_text'].apply(clean_text)\n",
    "\n",
    "# Tokenize and preprocess\n",
    "df['processed_tokens'] = df['review_text'].apply(preprocess_text)\n",
    "\n",
    "# Create processed text (join tokens back into text)\n",
    "df['processed_text'] = df['processed_tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "print(\"Text preprocessing completed!\")\n",
    "\n",
    "# Show before and after examples\n",
    "print(\"\\nBefore and After Preprocessing Examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {df.iloc[i]['review_text']}\")\n",
    "    print(f\"Cleaned: {df.iloc[i]['cleaned_text']}\")\n",
    "    print(f\"Tokens: {df.iloc[i]['processed_tokens']}\")\n",
    "    print(f\"Processed: {df.iloc[i]['processed_text']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fff4f3a",
   "metadata": {},
   "source": [
    "## Step 8: Tokenization Analysis\n",
    "\n",
    "Let's analyze the tokenization results and examine the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c9e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count tokens per review\n",
    "df['token_count'] = df['processed_tokens'].apply(len)\n",
    "\n",
    "print(\"Tokenization Statistics:\")\n",
    "print(f\"Average tokens per review: {df['token_count'].mean():.2f}\")\n",
    "print(f\"Min tokens per review: {df['token_count'].min()}\")\n",
    "print(f\"Max tokens per review: {df['token_count'].max()}\")\n",
    "\n",
    "# Collect all tokens\n",
    "all_tokens = []\n",
    "for tokens in df['processed_tokens']:\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "print(f\"\\nTotal unique tokens: {len(set(all_tokens))}\")\n",
    "print(f\"Total tokens: {len(all_tokens)}\")\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq = Counter(all_tokens)\n",
    "most_common_words = word_freq.most_common(20)\n",
    "\n",
    "print(\"\\nMost frequent words:\")\n",
    "for word, freq in most_common_words:\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6208e3",
   "metadata": {},
   "source": [
    "## Step 9: Visualize Most Frequent Words\n",
    "\n",
    "Let's create visualizations to explore the most frequent words in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a622ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word frequency visualization\n",
    "top_words = [word for word, freq in most_common_words[:15]]\n",
    "top_freqs = [freq for word, freq in most_common_words[:15]]\n",
    "\n",
    "# Bar plot of most frequent words\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(top_words, top_freqs, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 15 Most Frequent Words in Amazon Reviews')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height)}',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019dfd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word cloud\n",
    "# Join all processed text\n",
    "all_text = ' '.join(df['processed_text'])\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, \n",
    "                     background_color='white',\n",
    "                     colormap='viridis',\n",
    "                     max_words=100,\n",
    "                     relative_scaling=0.5,\n",
    "                     random_state=42).generate(all_text)\n",
    "\n",
    "# Display word cloud\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Amazon Reviews', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdba990",
   "metadata": {},
   "source": [
    "## Step 10: Interactive Visualization with Plotly\n",
    "\n",
    "Let's create interactive visualizations using Plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a91619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive bar chart of word frequencies\n",
    "fig = px.bar(x=top_words, y=top_freqs, \n",
    "             title='Top 15 Most Frequent Words (Interactive)',\n",
    "             labels={'x': 'Words', 'y': 'Frequency'},\n",
    "             color=top_freqs,\n",
    "             color_continuous_scale='viridis')\n",
    "\n",
    "fig.update_layout(showlegend=False, height=500)\n",
    "fig.update_traces(texttemplate='%{y}', textposition='outside')\n",
    "fig.show()\n",
    "\n",
    "# Interactive scatter plot of text length vs rating\n",
    "fig2 = px.scatter(df, x='text_length', y='rating', \n",
    "                  color='category',\n",
    "                  title='Review Text Length vs Rating by Category',\n",
    "                  labels={'text_length': 'Text Length (characters)', 'rating': 'Rating'},\n",
    "                  hover_data=['word_count', 'token_count'])\n",
    "\n",
    "fig2.update_layout(height=500)\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24135fb",
   "metadata": {},
   "source": [
    "## Step 11: Sentiment Analysis by Words\n",
    "\n",
    "Let's analyze positive and negative words in our reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18676566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define positive and negative words\n",
    "positive_words = ['amazing', 'excellent', 'great', 'good', 'love', 'perfect', 'outstanding', \n",
    "                 'fantastic', 'superb', 'wonderful', 'awesome', 'brilliant', 'satisfied']\n",
    "\n",
    "negative_words = ['poor', 'terrible', 'disappointed', 'horrible', 'bad', 'worst', 'awful', \n",
    "                 'waste', 'broken', 'damaged', 'unhelpful', 'misleading', 'overpriced']\n",
    "\n",
    "# Count positive and negative words in each review\n",
    "def count_sentiment_words(tokens, word_list):\n",
    "    return sum(1 for token in tokens if token in word_list)\n",
    "\n",
    "df['positive_words'] = df['processed_tokens'].apply(lambda x: count_sentiment_words(x, positive_words))\n",
    "df['negative_words'] = df['processed_tokens'].apply(lambda x: count_sentiment_words(x, negative_words))\n",
    "df['sentiment_score'] = df['positive_words'] - df['negative_words']\n",
    "\n",
    "# Analyze sentiment distribution\n",
    "print(\"Sentiment Analysis:\")\n",
    "print(f\"Average positive words per review: {df['positive_words'].mean():.2f}\")\n",
    "print(f\"Average negative words per review: {df['negative_words'].mean():.2f}\")\n",
    "print(f\"Average sentiment score: {df['sentiment_score'].mean():.2f}\")\n",
    "\n",
    "# Visualize sentiment vs rating\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Sentiment score vs rating\n",
    "scatter = axes[0].scatter(df['sentiment_score'], df['rating'], \n",
    "                         c=df['rating'], cmap='RdYlGn', \n",
    "                         alpha=0.7, s=100)\n",
    "axes[0].set_xlabel('Sentiment Score')\n",
    "axes[0].set_ylabel('Rating')\n",
    "axes[0].set_title('Sentiment Score vs Rating')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[0], label='Rating')\n",
    "\n",
    "# Positive vs negative words\n",
    "axes[1].scatter(df['positive_words'], df['negative_words'], \n",
    "               c=df['rating'], cmap='RdYlGn', \n",
    "               alpha=0.7, s=100)\n",
    "axes[1].set_xlabel('Positive Words Count')\n",
    "axes[1].set_ylabel('Negative Words Count')\n",
    "axes[1].set_title('Positive vs Negative Words')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cde4ad",
   "metadata": {},
   "source": [
    "## Step 12: Prepare for Vectorization\n",
    "\n",
    "Let's prepare our text data for machine learning by creating different vectorization approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576be76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text data for vectorization\n",
    "processed_texts = df['processed_text'].tolist()\n",
    "\n",
    "# 1. Count Vectorization\n",
    "print(\"1. Count Vectorization:\")\n",
    "count_vectorizer = CountVectorizer(max_features=100, ngram_range=(1, 2))\n",
    "count_matrix = count_vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "print(f\"Count matrix shape: {count_matrix.shape}\")\n",
    "print(f\"Feature names (first 10): {count_vectorizer.get_feature_names_out()[:10]}\")\n",
    "\n",
    "# 2. TF-IDF Vectorization\n",
    "print(\"\\n2. TF-IDF Vectorization:\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Feature names (first 10): {tfidf_vectorizer.get_feature_names_out()[:10]}\")\n",
    "\n",
    "# Show sample vectors\n",
    "print(\"\\nSample Count Vector (first review):\")\n",
    "print(count_matrix[0].toarray().flatten()[:20])\n",
    "\n",
    "print(\"\\nSample TF-IDF Vector (first review):\")\n",
    "print(tfidf_matrix[0].toarray().flatten()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfbaf18",
   "metadata": {},
   "source": [
    "## Step 13: Vocabulary Analysis\n",
    "\n",
    "Let's analyze the vocabulary created by our vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c121c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze vocabulary\n",
    "count_features = count_vectorizer.get_feature_names_out()\n",
    "tfidf_features = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Count vectorizer vocabulary size: {len(count_features)}\")\n",
    "print(f\"TF-IDF vectorizer vocabulary size: {len(tfidf_features)}\")\n",
    "\n",
    "# Get feature importance from TF-IDF\n",
    "feature_importance = tfidf_matrix.sum(axis=0).A1\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': tfidf_features,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 most important features (TF-IDF):\")\n",
    "print(feature_importance_df.head(15))\n",
    "\n",
    "# Visualize feature importance\n",
    "top_features = feature_importance_df.head(15)\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(top_features['feature'], top_features['importance'], \n",
    "               color='coral', edgecolor='darkred', alpha=0.7)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('TF-IDF Importance')\n",
    "plt.title('Top 15 Most Important Features (TF-IDF)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8626a2d",
   "metadata": {},
   "source": [
    "## Step 14: Final Analysis and Summary\n",
    "\n",
    "Let's create a comprehensive summary of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fd0516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"AMAZON REVIEWS ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä DATASET OVERVIEW:\")\n",
    "print(f\"‚Ä¢ Total reviews: {len(df)}\")\n",
    "print(f\"‚Ä¢ Average rating: {df['rating'].mean():.2f}\")\n",
    "print(f\"‚Ä¢ Rating distribution: {dict(df['rating'].value_counts().sort_index())}\")\n",
    "print(f\"‚Ä¢ Categories: {df['category'].unique()}\")\n",
    "\n",
    "print(f\"\\nüìù TEXT STATISTICS:\")\n",
    "print(f\"‚Ä¢ Average text length: {df['text_length'].mean():.0f} characters\")\n",
    "print(f\"‚Ä¢ Average word count: {df['word_count'].mean():.0f} words\")\n",
    "print(f\"‚Ä¢ Average tokens after preprocessing: {df['token_count'].mean():.0f}\")\n",
    "print(f\"‚Ä¢ Total unique vocabulary: {len(set(all_tokens))} words\")\n",
    "\n",
    "print(f\"\\nüí≠ SENTIMENT ANALYSIS:\")\n",
    "print(f\"‚Ä¢ Average positive words: {df['positive_words'].mean():.2f}\")\n",
    "print(f\"‚Ä¢ Average negative words: {df['negative_words'].mean():.2f}\")\n",
    "print(f\"‚Ä¢ Average sentiment score: {df['sentiment_score'].mean():.2f}\")\n",
    "\n",
    "print(f\"\\nüî§ MOST FREQUENT WORDS:\")\n",
    "for i, (word, freq) in enumerate(most_common_words[:10], 1):\n",
    "    print(f\"‚Ä¢ {i}. {word}: {freq} times\")\n",
    "\n",
    "print(f\"\\nüéØ VECTORIZATION RESULTS:\")\n",
    "print(f\"‚Ä¢ Count vectorizer features: {count_matrix.shape[1]}\")\n",
    "print(f\"‚Ä¢ TF-IDF vectorizer features: {tfidf_matrix.shape[1]}\")\n",
    "print(f\"‚Ä¢ Data ready for machine learning: ‚úÖ\")\n",
    "\n",
    "print(f\"\\nüéì LEARNING OBJECTIVES ACHIEVED:\")\n",
    "print(f\"‚Ä¢ ‚úÖ Loaded and explored text dataset\")\n",
    "print(f\"‚Ä¢ ‚úÖ Applied tokenization and preprocessing\")\n",
    "print(f\"‚Ä¢ ‚úÖ Removed stopwords successfully\")\n",
    "print(f\"‚Ä¢ ‚úÖ Visualized word frequencies and patterns\")\n",
    "print(f\"‚Ä¢ ‚úÖ Prepared data for vectorization\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS COMPLETE! üéâ\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f377c",
   "metadata": {},
   "source": [
    "## Step 15: Save Processed Data\n",
    "\n",
    "Let's save our processed data for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff6ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "output_file = 'data/processed_amazon_reviews.csv'\n",
    "os.makedirs('data', exist_ok=True)\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Processed data saved to: {output_file}\")\n",
    "\n",
    "# Save vectorizer models\n",
    "import pickle\n",
    "\n",
    "with open('data/count_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(count_vectorizer, f)\n",
    "    \n",
    "with open('data/tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "    \n",
    "print(\"Vectorizers saved successfully!\")\n",
    "\n",
    "# Create a summary report\n",
    "with open('data/analysis_summary.txt', 'w') as f:\n",
    "    f.write(\"Amazon Reviews Analysis Summary\\n\")\n",
    "    f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "    f.write(f\"Dataset size: {len(df)} reviews\\n\")\n",
    "    f.write(f\"Average rating: {df['rating'].mean():.2f}\\n\")\n",
    "    f.write(f\"Unique vocabulary: {len(set(all_tokens))} words\\n\")\n",
    "    f.write(f\"Most frequent word: {most_common_words[0][0]} ({most_common_words[0][1]} times)\\n\")\n",
    "    f.write(\"\\nTop 10 words:\\n\")\n",
    "    for word, freq in most_common_words[:10]:\n",
    "        f.write(f\"- {word}: {freq}\\n\")\n",
    "        \n",
    "print(\"Summary report saved to: data/analysis_summary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac15ebd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You have successfully completed the Amazon Reviews Analysis Laboratory. \n",
    "\n",
    "### What we accomplished:\n",
    "\n",
    "1. **Data Loading**: Loaded and explored a dataset of Amazon reviews\n",
    "2. **Text Preprocessing**: Cleaned text by removing special characters and converting to lowercase\n",
    "3. **Tokenization**: Split text into individual words/tokens\n",
    "4. **Stopword Removal**: Removed common English stopwords\n",
    "5. **Text Visualization**: Created word clouds and frequency plots\n",
    "6. **Vectorization**: Prepared text data for machine learning using Count and TF-IDF vectorizers\n",
    "7. **Sentiment Analysis**: Analyzed positive and negative sentiment words\n",
    "8. **Data Export**: Saved processed data and models for future use\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "The processed data is now ready for:\n",
    "- **Machine Learning**: Classification, clustering, or regression models\n",
    "- **Sentiment Analysis**: Advanced sentiment classification\n",
    "- **Topic Modeling**: Discovering themes in reviews\n",
    "- **Recommendation Systems**: Product recommendation based on reviews\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- Text preprocessing is crucial for NLP tasks\n",
    "- Tokenization and stopword removal significantly reduce noise\n",
    "- Visualization helps understand text patterns\n",
    "- Vectorization transforms text into numerical format for ML\n",
    "- Different vectorization methods serve different purposes\n",
    "\n",
    "**Great job completing this laboratory! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
