{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72a81db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this cell first if packages are missing)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"‚úÖ {package} installed successfully\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"‚ùå Failed to install {package}\")\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"yfinance>=0.2.0\",\n",
    "    \"stable-baselines3>=2.0.0\", \n",
    "    \"gymnasium>=0.29.0\",\n",
    "    \"gym>=0.21.0\",\n",
    "    \"matplotlib>=3.7.0\",\n",
    "    \"seaborn>=0.12.0\",\n",
    "    \"plotly>=5.15.0\",\n",
    "    \"numpy>=1.24.0\",\n",
    "    \"pandas>=2.0.0\"\n",
    "]\n",
    "\n",
    "print(\"üîß Installing required packages...\")\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package.split(\">=\")[0].replace(\"-\", \"_\"))\n",
    "        print(f\"‚úÖ {package.split('>=')[0]} already available\")\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ Installing {package}...\")\n",
    "        install_package(package)\n",
    "\n",
    "print(\"üéâ Package installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f277856",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Asset Allocation\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "In this laboratory, we'll develop a **Reinforcement Learning agent** that learns to optimally allocate a portfolio of assets to maximize cumulative returns while considering:\n",
    "- **Risk management** (volatility, drawdown)\n",
    "- **Transaction costs** (realistic trading friction)\n",
    "- **Market dynamics** (changing market conditions)\n",
    "\n",
    "## ü§ñ Why Reinforcement Learning for Finance?\n",
    "\n",
    "Unlike supervised learning, which predicts future prices, RL focuses on **sequential decision-making**:\n",
    "\n",
    "| Approach | Goal | Challenge |\n",
    "|----------|------|----------|\n",
    "| **Supervised Learning** | Predict next price | Prediction ‚â† Optimal trading |\n",
    "| **Reinforcement Learning** | Learn optimal actions | Direct optimization of trading performance |\n",
    "\n",
    "**Key advantages of RL:**\n",
    "- Learns from **trial and error** in market simulation\n",
    "- Optimizes **long-term cumulative rewards**\n",
    "- Naturally handles **sequential dependencies**\n",
    "- Can incorporate **risk constraints** and **transaction costs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0c354",
   "metadata": {},
   "source": [
    "## üìö Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1520b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Financial data\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# RL libraries\n",
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Plotting\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "# Set style\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except OSError:\n",
    "    # Fallback for newer matplotlib versions\n",
    "    plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16806217",
   "metadata": {},
   "source": [
    "## üìà Data Collection and Preprocessing\n",
    "\n",
    "We'll use **Yahoo Finance** to download historical data for a diversified portfolio of assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860fe9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our asset universe\n",
    "ASSETS = ['SPY', 'AAPL', 'MSFT', 'GOOGL']\n",
    "START_DATE = '2018-01-01'\n",
    "END_DATE = '2023-12-31'\n",
    "SPLIT_DATE = '2022-01-01'  # Train/test split\n",
    "\n",
    "print(f\"üìä Downloading data for {len(ASSETS)} assets: {', '.join(ASSETS)}\")\n",
    "print(f\"üìÖ Period: {START_DATE} to {END_DATE}\")\n",
    "print(f\"üîÑ Train/Test split: {SPLIT_DATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880f6b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(symbols, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Download and preprocess financial data from Yahoo Finance\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Downloading data...\")\n",
    "    \n",
    "    # Download data\n",
    "    data = yf.download(symbols, start=start_date, end=end_date, progress=False)\n",
    "    \n",
    "    # Check if data was downloaded successfully\n",
    "    if data.empty:\n",
    "        raise ValueError(\"No data downloaded. Please check symbols and date range.\")\n",
    "    \n",
    "    print(f\"üìä Data structure: {data.shape}\")\n",
    "    print(f\"üìä Columns: {data.columns.tolist()}\")\n",
    "    print(f\"üìä Column levels: {data.columns.nlevels}\")\n",
    "    \n",
    "    # Handle different data structures based on number of symbols\n",
    "    if len(symbols) == 1:\n",
    "        # Single symbol case - data is a simple DataFrame\n",
    "        if 'Adj Close' in data.columns:\n",
    "            prices = data['Adj Close'].to_frame()\n",
    "        else:\n",
    "            prices = data['Close'].to_frame()\n",
    "        prices.columns = symbols\n",
    "    else:\n",
    "        # Multiple symbols case - check if we have MultiIndex columns\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            # Check available columns\n",
    "            available_columns = data.columns.get_level_values(0).unique()\n",
    "            print(f\"üìä Available price types: {available_columns.tolist()}\")\n",
    "            \n",
    "            if 'Adj Close' in available_columns:\n",
    "                prices = data['Adj Close'].copy()\n",
    "            elif 'Close' in available_columns:\n",
    "                prices = data['Close'].copy()\n",
    "            else:\n",
    "                # Use the first available price column\n",
    "                price_col = available_columns[0]\n",
    "                prices = data[price_col].copy()\n",
    "                print(f\"‚ö†Ô∏è Using {price_col} as no Close/Adj Close found\")\n",
    "        else:\n",
    "            # Simple columns structure\n",
    "            prices = data.copy()\n",
    "    \n",
    "    # Remove any missing data\n",
    "    prices = prices.dropna()\n",
    "    \n",
    "    # Calculate returns\n",
    "    returns = prices.pct_change().dropna()\n",
    "    \n",
    "    # Calculate technical indicators\n",
    "    tech_indicators = pd.DataFrame(index=prices.index)\n",
    "    \n",
    "    for asset in prices.columns:\n",
    "        # Simple moving averages\n",
    "        tech_indicators[f'{asset}_SMA_10'] = prices[asset].rolling(10).mean() / prices[asset] - 1\n",
    "        tech_indicators[f'{asset}_SMA_30'] = prices[asset].rolling(30).mean() / prices[asset] - 1\n",
    "        \n",
    "        # Volatility (rolling std of returns)\n",
    "        tech_indicators[f'{asset}_VOL_10'] = returns[asset].rolling(10).std()\n",
    "        \n",
    "        # RSI-like momentum indicator\n",
    "        tech_indicators[f'{asset}_MOMENTUM'] = returns[asset].rolling(5).sum()\n",
    "    \n",
    "    # Remove NaN values\n",
    "    tech_indicators = tech_indicators.dropna()\n",
    "    \n",
    "    print(f\"‚úÖ Downloaded {len(prices)} days of data\")\n",
    "    print(f\"üìä Assets: {list(prices.columns)}\")\n",
    "    \n",
    "    return prices, returns, tech_indicators\n",
    "\n",
    "# Download the data\n",
    "prices, returns, tech_indicators = download_data(ASSETS, START_DATE, END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e9e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "print(\"üìä Asset Price Statistics\")\n",
    "print(\"=\" * 50)\n",
    "display(prices.describe())\n",
    "\n",
    "print(\"\\nüìà Daily Returns Statistics\")\n",
    "print(\"=\" * 50)\n",
    "display(returns.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dae965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize price evolution\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=ASSETS,\n",
    "    vertical_spacing=0.1\n",
    ")\n",
    "\n",
    "# Normalize prices to start at 100 for comparison\n",
    "normalized_prices = (prices / prices.iloc[0]) * 100\n",
    "\n",
    "for i, asset in enumerate(ASSETS):\n",
    "    row = (i // 2) + 1\n",
    "    col = (i % 2) + 1\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=normalized_prices.index,\n",
    "            y=normalized_prices[asset],\n",
    "            name=asset,\n",
    "            line=dict(width=2)\n",
    "        ),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    title_text=\"üìà Asset Price Evolution (Normalized to 100)\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2a979",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Portfolio Management Environment\n",
    "\n",
    "We'll create a custom **OpenAI Gym environment** for portfolio management. The agent will learn to:\n",
    "- **Observe** market conditions and current portfolio state\n",
    "- **Act** by choosing new portfolio allocations\n",
    "- **Receive rewards** based on risk-adjusted returns minus costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0978d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioEnv(gymnasium.Env):\n",
    "    \"\"\"\n",
    "    Custom Portfolio Management Environment\n",
    "    \n",
    "    State: [current_weights, tech_indicators, returns_history]\n",
    "    Action: New portfolio weights (must sum to 1)\n",
    "    Reward: Risk-adjusted returns minus transaction costs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, prices, returns, tech_indicators, \n",
    "                 initial_balance=100000, transaction_cost=0.001,\n",
    "                 lookback_window=10):\n",
    "        super(PortfolioEnv, self).__init__()\n",
    "        \n",
    "        self.prices = prices\n",
    "        self.returns = returns\n",
    "        self.tech_indicators = tech_indicators\n",
    "        self.initial_balance = initial_balance\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.lookback_window = lookback_window\n",
    "        \n",
    "        # Asset information\n",
    "        self.n_assets = len(prices.columns)\n",
    "        self.asset_names = list(prices.columns)\n",
    "        \n",
    "        # Find common dates across all dataframes\n",
    "        common_dates = prices.index.intersection(returns.index).intersection(tech_indicators.index)\n",
    "        self.dates = sorted(common_dates)\n",
    "        \n",
    "        # Filter data to common dates\n",
    "        self.prices = self.prices.loc[self.dates]\n",
    "        self.returns = self.returns.loc[self.dates]\n",
    "        self.tech_indicators = self.tech_indicators.loc[self.dates]\n",
    "        \n",
    "        # Define action and observation spaces\n",
    "        # Action: portfolio weights (continuous, must sum to 1)\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)\n",
    "        \n",
    "        # Observation: current weights + tech indicators + recent returns\n",
    "        n_tech_features = len(self.tech_indicators.columns)\n",
    "        n_return_features = self.n_assets * self.lookback_window\n",
    "        obs_dim = self.n_assets + n_tech_features + n_return_features\n",
    "        \n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Initialize state\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Reset environment to initial state\"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        self.current_step = self.lookback_window\n",
    "        self.balance = self.initial_balance\n",
    "        \n",
    "        # Start with equal weight portfolio\n",
    "        self.weights = np.ones(self.n_assets) / self.n_assets\n",
    "        \n",
    "        # Track portfolio value history\n",
    "        self.portfolio_values = [self.initial_balance]\n",
    "        self.weight_history = [self.weights.copy()]\n",
    "        \n",
    "        observation = self._get_observation()\n",
    "        info = {}\n",
    "        \n",
    "        return observation, info\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Get current observation state\"\"\"\n",
    "        if self.current_step >= len(self.dates):\n",
    "            return np.zeros(self.observation_space.shape[0], dtype=np.float32)\n",
    "        \n",
    "        # Current portfolio weights\n",
    "        obs = list(self.weights)\n",
    "        \n",
    "        # Technical indicators\n",
    "        current_date = self.dates[self.current_step]\n",
    "        tech_values = self.tech_indicators.loc[current_date].values\n",
    "        obs.extend(tech_values)\n",
    "        \n",
    "        # Recent returns history\n",
    "        start_idx = max(0, self.current_step - self.lookback_window)\n",
    "        recent_returns = self.returns.iloc[start_idx:self.current_step].values\n",
    "        \n",
    "        # Flatten and pad if necessary\n",
    "        recent_returns_flat = recent_returns.flatten()\n",
    "        expected_length = self.n_assets * self.lookback_window\n",
    "        \n",
    "        if len(recent_returns_flat) < expected_length:\n",
    "            # Pad with zeros if we don't have enough history\n",
    "            padding = np.zeros(expected_length - len(recent_returns_flat))\n",
    "            recent_returns_flat = np.concatenate([padding, recent_returns_flat])\n",
    "        \n",
    "        obs.extend(recent_returns_flat)\n",
    "        \n",
    "        return np.array(obs, dtype=np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one step in the environment\"\"\"\n",
    "        if self.current_step >= len(self.dates) - 1:\n",
    "            return self._get_observation(), 0, True, True, {}\n",
    "        \n",
    "        # Normalize action to ensure weights sum to 1\n",
    "        new_weights = np.array(action)\n",
    "        new_weights = np.clip(new_weights, 0, 1)\n",
    "        new_weights = new_weights / (new_weights.sum() + 1e-8)\n",
    "        \n",
    "        # Calculate transaction costs\n",
    "        weight_changes = np.abs(new_weights - self.weights)\n",
    "        transaction_costs = np.sum(weight_changes) * self.transaction_cost\n",
    "        \n",
    "        # Update weights\n",
    "        self.weights = new_weights\n",
    "        \n",
    "        # Calculate portfolio return\n",
    "        current_date = self.dates[self.current_step]\n",
    "        asset_returns = self.returns.loc[current_date].values\n",
    "        portfolio_return = np.dot(self.weights, asset_returns)\n",
    "        \n",
    "        # Update portfolio value\n",
    "        self.balance = self.balance * (1 + portfolio_return - transaction_costs)\n",
    "        self.portfolio_values.append(self.balance)\n",
    "        self.weight_history.append(self.weights.copy())\n",
    "        \n",
    "        # Calculate reward (Sharpe-like ratio with transaction cost penalty)\n",
    "        if len(self.portfolio_values) >= 20:  # Need some history\n",
    "            recent_values = np.array(self.portfolio_values[-20:])\n",
    "            recent_returns = np.diff(recent_values) / recent_values[:-1]\n",
    "            mean_return = np.mean(recent_returns)\n",
    "            std_return = np.std(recent_returns) + 1e-8\n",
    "            sharpe_ratio = mean_return / std_return\n",
    "            reward = sharpe_ratio - transaction_costs * 100  # Scale transaction cost penalty\n",
    "        else:\n",
    "            reward = portfolio_return - transaction_costs * 100\n",
    "        \n",
    "        # Move to next step\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Check if episode is done\n",
    "        terminated = self.current_step >= len(self.dates) - 1\n",
    "        truncated = False  # We don't truncate episodes\n",
    "        \n",
    "        info = {\n",
    "            'portfolio_value': self.balance,\n",
    "            'portfolio_return': portfolio_return,\n",
    "            'transaction_costs': transaction_costs,\n",
    "            'weights': self.weights.copy()\n",
    "        }\n",
    "        \n",
    "        return self._get_observation(), reward, terminated, truncated, info\n",
    "    \n",
    "    def get_portfolio_stats(self):\n",
    "        \"\"\"Calculate portfolio performance statistics\"\"\"\n",
    "        if len(self.portfolio_values) < 2:\n",
    "            return {}\n",
    "        \n",
    "        values = np.array(self.portfolio_values)\n",
    "        returns = np.diff(values) / values[:-1]\n",
    "        \n",
    "        total_return = (values[-1] / values[0]) - 1\n",
    "        annualized_return = (1 + total_return) ** (252 / len(returns)) - 1\n",
    "        volatility = np.std(returns) * np.sqrt(252)\n",
    "        sharpe_ratio = annualized_return / volatility if volatility > 0 else 0\n",
    "        \n",
    "        # Maximum drawdown\n",
    "        cumulative = values / np.maximum.accumulate(values)\n",
    "        max_drawdown = (1 - np.min(cumulative))\n",
    "        \n",
    "        return {\n",
    "            'total_return': total_return,\n",
    "            'annualized_return': annualized_return,\n",
    "            'volatility': volatility,\n",
    "            'sharpe_ratio': sharpe_ratio,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'final_value': values[-1]\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Portfolio Environment created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2574577f",
   "metadata": {},
   "source": [
    "## üß™ Environment Setup and Testing\n",
    "\n",
    "Let's create training and testing environments and verify they work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903861ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing periods\n",
    "# Note: returns and tech_indicators have different lengths due to pct_change() and rolling calculations\n",
    "train_mask_prices = prices.index < SPLIT_DATE\n",
    "test_mask_prices = prices.index >= SPLIT_DATE\n",
    "\n",
    "train_mask_returns = returns.index < SPLIT_DATE\n",
    "test_mask_returns = returns.index >= SPLIT_DATE\n",
    "\n",
    "train_mask_tech = tech_indicators.index < SPLIT_DATE\n",
    "test_mask_tech = tech_indicators.index >= SPLIT_DATE\n",
    "\n",
    "# Training data\n",
    "train_prices = prices[train_mask_prices]\n",
    "train_returns = returns[train_mask_returns]\n",
    "train_tech = tech_indicators[train_mask_tech]\n",
    "\n",
    "# Testing data\n",
    "test_prices = prices[test_mask_prices]\n",
    "test_returns = returns[test_mask_returns]\n",
    "test_tech = tech_indicators[test_mask_tech]\n",
    "\n",
    "print(f\"üìä Training period: {train_prices.index[0].date()} to {train_prices.index[-1].date()}\")\n",
    "print(f\"üìä Testing period: {test_prices.index[0].date()} to {test_prices.index[-1].date()}\")\n",
    "print(f\"üìà Training days: {len(train_prices)}\")\n",
    "print(f\"üìà Testing days: {len(test_prices)}\")\n",
    "print(f\"üìà Training returns: {len(train_returns)} | tech indicators: {len(train_tech)}\")\n",
    "print(f\"üìà Testing returns: {len(test_returns)} | tech indicators: {len(test_tech)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4658bb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environments\n",
    "train_env = PortfolioEnv(\n",
    "    prices=train_prices,\n",
    "    returns=train_returns,\n",
    "    tech_indicators=train_tech,\n",
    "    initial_balance=100000,\n",
    "    transaction_cost=0.001\n",
    ")\n",
    "\n",
    "test_env = PortfolioEnv(\n",
    "    prices=test_prices,\n",
    "    returns=test_returns,\n",
    "    tech_indicators=test_tech,\n",
    "    initial_balance=100000,\n",
    "    transaction_cost=0.001\n",
    ")\n",
    "\n",
    "# Verify environment\n",
    "print(\"üîç Checking environment...\")\n",
    "check_env(train_env)\n",
    "print(\"‚úÖ Environment passed all checks!\")\n",
    "\n",
    "print(f\"\\nüìä Environment Details:\")\n",
    "print(f\"   ‚Ä¢ Assets: {train_env.asset_names}\")\n",
    "print(f\"   ‚Ä¢ Observation space: {train_env.observation_space.shape}\")\n",
    "print(f\"   ‚Ä¢ Action space: {train_env.action_space.shape}\")\n",
    "print(f\"   ‚Ä¢ Training episodes: {len(train_env.dates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a222766",
   "metadata": {},
   "source": [
    "## üìä Baseline Strategies\n",
    "\n",
    "Before training our RL agent, let's implement baseline strategies to compare against:\n",
    "1. **Buy and Hold**: Invest equally and never rebalance\n",
    "2. **Equal Weight**: Rebalance to equal weights periodically\n",
    "3. **Random**: Random allocations (worst case scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25961af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_baseline_strategy(env, strategy='equal_weight', rebalance_freq=20):\n",
    "    \"\"\"\n",
    "    Evaluate baseline strategies\n",
    "    \n",
    "    Strategies:\n",
    "    - 'buy_hold': Buy and hold equal weights\n",
    "    - 'equal_weight': Rebalance to equal weights periodically\n",
    "    - 'random': Random allocations\n",
    "    \"\"\"\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    \n",
    "    # Initial equal weights\n",
    "    equal_weights = np.ones(env.n_assets) / env.n_assets\n",
    "    \n",
    "    while not done:\n",
    "        if strategy == 'buy_hold':\n",
    "            # Never rebalance, keep current weights\n",
    "            action = env.weights\n",
    "        \n",
    "        elif strategy == 'equal_weight':\n",
    "            # Rebalance to equal weights periodically\n",
    "            if step_count % rebalance_freq == 0:\n",
    "                action = equal_weights\n",
    "            else:\n",
    "                action = env.weights\n",
    "        \n",
    "        elif strategy == 'random':\n",
    "            # Random weights\n",
    "            action = np.random.random(env.n_assets)\n",
    "            action = action / action.sum()\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "        \n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        step_count += 1\n",
    "    \n",
    "    return env.get_portfolio_stats(), env.portfolio_values, env.weight_history\n",
    "\n",
    "print(\"üìä Evaluating baseline strategies...\")\n",
    "\n",
    "# Evaluate on training data\n",
    "strategies = ['buy_hold', 'equal_weight', 'random']\n",
    "baseline_results = {}\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"   ‚Ä¢ Evaluating {strategy} strategy...\")\n",
    "    stats, values, weights = evaluate_baseline_strategy(train_env, strategy)\n",
    "    baseline_results[strategy] = {\n",
    "        'stats': stats,\n",
    "        'values': values,\n",
    "        'weights': weights\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Baseline evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09a7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display baseline results\n",
    "baseline_df = pd.DataFrame({\n",
    "    strategy: results['stats'] \n",
    "    for strategy, results in baseline_results.items()\n",
    "}).T\n",
    "\n",
    "print(\"üìä Baseline Strategy Performance (Training Period)\")\n",
    "print(\"=\" * 60)\n",
    "display(baseline_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd437ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot baseline performance\n",
    "fig = go.Figure()\n",
    "\n",
    "for strategy, results in baseline_results.items():\n",
    "    values = results['values']\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(values))),\n",
    "        y=values,\n",
    "        name=strategy.replace('_', ' ').title(),\n",
    "        line=dict(width=2)\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"üìà Baseline Strategy Performance (Training Period)\",\n",
    "    xaxis_title=\"Days\",\n",
    "    yaxis_title=\"Portfolio Value ($)\",\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeef7c6",
   "metadata": {},
   "source": [
    "## ü§ñ Reinforcement Learning Agent Training\n",
    "\n",
    "Now let's train our RL agent using **PPO (Proximal Policy Optimization)**, which is well-suited for continuous action spaces like portfolio allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc878393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorized environment for stable-baselines3\n",
    "vec_env = DummyVecEnv([lambda: train_env])\n",
    "\n",
    "# Initialize PPO agent\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    verbose=1,\n",
    "    tensorboard_log=None  # Disable tensorboard logging\n",
    ")\n",
    "\n",
    "print(\"ü§ñ PPO Agent initialized!\")\n",
    "print(f\"   ‚Ä¢ Policy: MLP (Multi-Layer Perceptron)\")\n",
    "print(f\"   ‚Ä¢ Learning rate: {model.learning_rate}\")\n",
    "print(f\"   ‚Ä¢ Batch size: {model.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4772976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"   This may take a few minutes...\")\n",
    "\n",
    "# Train for multiple episodes\n",
    "total_timesteps = 50000\n",
    "model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "print(\"‚úÖ Training completed!\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"data/ppo_portfolio_agent\")\n",
    "print(\"üíæ Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb516bb",
   "metadata": {},
   "source": [
    "## üìà Agent Evaluation and Testing\n",
    "\n",
    "Let's evaluate our trained agent on both training and testing data to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e711a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rl_agent(model, env, deterministic=True):\n",
    "    \"\"\"\n",
    "    Evaluate the trained RL agent\n",
    "    \"\"\"\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=deterministic)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    return env.get_portfolio_stats(), env.portfolio_values, env.weight_history\n",
    "\n",
    "print(\"üß™ Evaluating RL agent...\")\n",
    "\n",
    "# Evaluate on training data\n",
    "train_stats, train_values, train_weights = evaluate_rl_agent(model, train_env)\n",
    "\n",
    "# Evaluate on testing data (out-of-sample)\n",
    "test_stats, test_values, test_weights = evaluate_rl_agent(model, test_env)\n",
    "\n",
    "print(\"‚úÖ RL agent evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f122a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare RL agent with baselines\n",
    "print(\"üìä RL Agent vs Baselines (Training Data)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Add RL results to comparison\n",
    "comparison_data = {\n",
    "    'Buy & Hold': baseline_results['buy_hold']['stats'],\n",
    "    'Equal Weight': baseline_results['equal_weight']['stats'],\n",
    "    'Random': baseline_results['random']['stats'],\n",
    "    'RL Agent': train_stats\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data).T\n",
    "display(comparison_df.round(4))\n",
    "\n",
    "print(\"\\nüìä RL Agent Performance (Testing Data - Out of Sample)\")\n",
    "print(\"=\" * 50)\n",
    "test_df = pd.DataFrame({'RL Agent (Test)': test_stats}, index=[0]).T\n",
    "display(test_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e601ca1",
   "metadata": {},
   "source": [
    "## üìä Performance Visualization and Analysis\n",
    "\n",
    "Let's create comprehensive visualizations to analyze the performance of our RL agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d3626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio value comparison\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=['Training Period Performance', 'Testing Period Performance'],\n",
    "    vertical_spacing=0.15\n",
    ")\n",
    "\n",
    "# Training period\n",
    "for strategy, results in baseline_results.items():\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(len(results['values']))),\n",
    "            y=results['values'],\n",
    "            name=strategy.replace('_', ' ').title(),\n",
    "            line=dict(width=2)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(train_values))),\n",
    "        y=train_values,\n",
    "        name='RL Agent (Train)',\n",
    "        line=dict(width=3, color='red')\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Testing period\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(test_values))),\n",
    "        y=test_values,\n",
    "        name='RL Agent (Test)',\n",
    "        line=dict(width=3, color='darkred')\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"üöÄ Portfolio Performance: RL Agent vs Baselines\",\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Days\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Portfolio Value ($)\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7484958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio allocation heatmap\n",
    "def plot_allocation_heatmap(weights_history, asset_names, title):\n",
    "    \"\"\"\n",
    "    Plot portfolio allocation over time as a heatmap\n",
    "    \"\"\"\n",
    "    weights_df = pd.DataFrame(weights_history, columns=asset_names)\n",
    "    \n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=weights_df.T.values,\n",
    "        x=list(range(len(weights_df))),\n",
    "        y=asset_names,\n",
    "        colorscale='RdYlBu_r',\n",
    "        colorbar=dict(title=\"Weight\")\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Time Steps\",\n",
    "        yaxis_title=\"Assets\",\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Plot allocation heatmaps\n",
    "fig1 = plot_allocation_heatmap(\n",
    "    train_weights, \n",
    "    ASSETS, \n",
    "    \"üéØ RL Agent Portfolio Allocation (Training)\"\n",
    ")\n",
    "fig1.show()\n",
    "\n",
    "fig2 = plot_allocation_heatmap(\n",
    "    test_weights, \n",
    "    ASSETS, \n",
    "    \"üéØ RL Agent Portfolio Allocation (Testing)\"\n",
    ")\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3f35c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk-Return scatter plot\n",
    "def calculate_risk_return_metrics(values):\n",
    "    \"\"\"Calculate annualized return and volatility\"\"\"\n",
    "    if len(values) < 2:\n",
    "        return 0, 0\n",
    "    \n",
    "    returns = np.diff(values) / values[:-1]\n",
    "    total_return = (values[-1] / values[0]) - 1\n",
    "    annualized_return = (1 + total_return) ** (252 / len(returns)) - 1\n",
    "    volatility = np.std(returns) * np.sqrt(252)\n",
    "    \n",
    "    return annualized_return, volatility\n",
    "\n",
    "# Calculate metrics for all strategies\n",
    "risk_return_data = []\n",
    "\n",
    "# Baselines\n",
    "for strategy, results in baseline_results.items():\n",
    "    ann_return, volatility = calculate_risk_return_metrics(results['values'])\n",
    "    risk_return_data.append({\n",
    "        'Strategy': strategy.replace('_', ' ').title(),\n",
    "        'Return': ann_return,\n",
    "        'Volatility': volatility,\n",
    "        'Type': 'Baseline'\n",
    "    })\n",
    "\n",
    "# RL Agent\n",
    "train_return, train_vol = calculate_risk_return_metrics(train_values)\n",
    "test_return, test_vol = calculate_risk_return_metrics(test_values)\n",
    "\n",
    "risk_return_data.extend([\n",
    "    {\n",
    "        'Strategy': 'RL Agent (Train)',\n",
    "        'Return': train_return,\n",
    "        'Volatility': train_vol,\n",
    "        'Type': 'RL Agent'\n",
    "    },\n",
    "    {\n",
    "        'Strategy': 'RL Agent (Test)',\n",
    "        'Return': test_return,\n",
    "        'Volatility': test_vol,\n",
    "        'Type': 'RL Agent'\n",
    "    }\n",
    "])\n",
    "\n",
    "risk_return_df = pd.DataFrame(risk_return_data)\n",
    "\n",
    "# Plot risk-return scatter\n",
    "fig = px.scatter(\n",
    "    risk_return_df,\n",
    "    x='Volatility',\n",
    "    y='Return',\n",
    "    color='Type',\n",
    "    text='Strategy',\n",
    "    title=\"üìä Risk-Return Analysis: All Strategies\",\n",
    "    labels={\n",
    "        'Return': 'Annualized Return',\n",
    "        'Volatility': 'Annualized Volatility'\n",
    "    }\n",
    ")\n",
    "\n",
    "fig.update_traces(textposition=\"top center\")\n",
    "fig.update_layout(height=500)\n",
    "fig.show()\n",
    "\n",
    "print(\"üìà Risk-Return Metrics\")\n",
    "print(\"=\" * 40)\n",
    "display(risk_return_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c85a8e",
   "metadata": {},
   "source": [
    "## üìã Performance Summary and Analysis\n",
    "\n",
    "Let's create a comprehensive summary of our results and analyze what the RL agent learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance table\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Total Return (%)',\n",
    "        'Annualized Return (%)',\n",
    "        'Volatility (%)',\n",
    "        'Sharpe Ratio',\n",
    "        'Max Drawdown (%)',\n",
    "        'Final Value ($)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Add baseline results\n",
    "for strategy in ['buy_hold', 'equal_weight']:\n",
    "    stats = baseline_results[strategy]['stats']\n",
    "    summary_data[strategy.replace('_', ' ').title()] = [\n",
    "        f\"{stats['total_return']*100:.2f}%\",\n",
    "        f\"{stats['annualized_return']*100:.2f}%\",\n",
    "        f\"{stats['volatility']*100:.2f}%\",\n",
    "        f\"{stats['sharpe_ratio']:.3f}\",\n",
    "        f\"{stats['max_drawdown']*100:.2f}%\",\n",
    "        f\"${stats['final_value']:,.0f}\"\n",
    "    ]\n",
    "\n",
    "# Add RL results\n",
    "summary_data['RL Agent (Train)'] = [\n",
    "    f\"{train_stats['total_return']*100:.2f}%\",\n",
    "    f\"{train_stats['annualized_return']*100:.2f}%\",\n",
    "    f\"{train_stats['volatility']*100:.2f}%\",\n",
    "    f\"{train_stats['sharpe_ratio']:.3f}\",\n",
    "    f\"{train_stats['max_drawdown']*100:.2f}%\",\n",
    "    f\"${train_stats['final_value']:,.0f}\"\n",
    "]\n",
    "\n",
    "summary_data['RL Agent (Test)'] = [\n",
    "    f\"{test_stats['total_return']*100:.2f}%\",\n",
    "    f\"{test_stats['annualized_return']*100:.2f}%\",\n",
    "    f\"{test_stats['volatility']*100:.2f}%\",\n",
    "    f\"{test_stats['sharpe_ratio']:.3f}\",\n",
    "    f\"{test_stats['max_drawdown']*100:.2f}%\",\n",
    "    f\"${test_stats['final_value']:,.0f}\"\n",
    "]\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.set_index('Metric', inplace=True)\n",
    "\n",
    "print(\"üèÜ FINAL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db59684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze allocation patterns\n",
    "print(\"üéØ Portfolio Allocation Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Average allocations\n",
    "train_weights_df = pd.DataFrame(train_weights, columns=ASSETS)\n",
    "test_weights_df = pd.DataFrame(test_weights, columns=ASSETS)\n",
    "\n",
    "print(\"üìä Average Asset Allocations:\")\n",
    "print(f\"Training Period:\")\n",
    "train_avg = train_weights_df.mean()\n",
    "for asset, weight in train_avg.items():\n",
    "    print(f\"   ‚Ä¢ {asset}: {weight:.1%}\")\n",
    "\n",
    "print(f\"\\nTesting Period:\")\n",
    "test_avg = test_weights_df.mean()\n",
    "for asset, weight in test_avg.items():\n",
    "    print(f\"   ‚Ä¢ {asset}: {weight:.1%}\")\n",
    "\n",
    "# Allocation volatility (how much the agent rebalances)\n",
    "print(f\"\\nüìà Allocation Volatility (Rebalancing Frequency):\")\n",
    "train_vol = train_weights_df.std()\n",
    "test_vol = test_weights_df.std()\n",
    "\n",
    "print(f\"Training Period:\")\n",
    "for asset, vol in train_vol.items():\n",
    "    print(f\"   ‚Ä¢ {asset}: {vol:.3f}\")\n",
    "\n",
    "print(f\"Testing Period:\")\n",
    "for asset, vol in test_vol.items():\n",
    "    print(f\"   ‚Ä¢ {asset}: {vol:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa21e674",
   "metadata": {},
   "source": [
    "## üîç Key Insights and Observations\n",
    "\n",
    "Let's analyze what our RL agent learned and identify key insights from the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison insights\n",
    "print(\"üß† KEY INSIGHTS FROM RL ASSET ALLOCATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compare returns\n",
    "buy_hold_return = baseline_results['buy_hold']['stats']['total_return']\n",
    "equal_weight_return = baseline_results['equal_weight']['stats']['total_return']\n",
    "rl_train_return = train_stats['total_return']\n",
    "rl_test_return = test_stats['total_return']\n",
    "\n",
    "print(\"üìà RETURN ANALYSIS:\")\n",
    "print(f\"   ‚Ä¢ Buy & Hold achieved {buy_hold_return:.1%} total return\")\n",
    "print(f\"   ‚Ä¢ Equal Weight achieved {equal_weight_return:.1%} total return\")\n",
    "print(f\"   ‚Ä¢ RL Agent achieved {rl_train_return:.1%} (train) / {rl_test_return:.1%} (test)\")\n",
    "\n",
    "if rl_test_return > buy_hold_return:\n",
    "    print(f\"   ‚úÖ RL Agent outperformed Buy & Hold by {(rl_test_return - buy_hold_return):.1%}\")\n",
    "else:\n",
    "    print(f\"   ‚ùå RL Agent underperformed Buy & Hold by {(buy_hold_return - rl_test_return):.1%}\")\n",
    "\n",
    "# Compare Sharpe ratios\n",
    "buy_hold_sharpe = baseline_results['buy_hold']['stats']['sharpe_ratio']\n",
    "rl_test_sharpe = test_stats['sharpe_ratio']\n",
    "\n",
    "print(f\"\\nüìä RISK-ADJUSTED RETURNS (Sharpe Ratio):\")\n",
    "print(f\"   ‚Ä¢ Buy & Hold: {buy_hold_sharpe:.3f}\")\n",
    "print(f\"   ‚Ä¢ RL Agent (Test): {rl_test_sharpe:.3f}\")\n",
    "\n",
    "if rl_test_sharpe > buy_hold_sharpe:\n",
    "    print(f\"   ‚úÖ RL Agent achieved better risk-adjusted returns\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Buy & Hold had better risk-adjusted returns\")\n",
    "\n",
    "# Analyze overfitting\n",
    "performance_gap = abs(rl_train_return - rl_test_return)\n",
    "print(f\"\\nüéØ OVERFITTING ANALYSIS:\")\n",
    "print(f\"   ‚Ä¢ Train vs Test performance gap: {performance_gap:.1%}\")\n",
    "if performance_gap > 0.05:  # 5% threshold\n",
    "    print(f\"   ‚ö†Ô∏è  Potential overfitting detected (gap > 5%)\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Good generalization (gap < 5%)\")\n",
    "\n",
    "# Transaction cost impact\n",
    "avg_rebalancing = np.mean([np.std(train_weights_df.iloc[i] - train_weights_df.iloc[i-1]) \n",
    "                          for i in range(1, len(train_weights_df))])\n",
    "print(f\"\\nüí∞ TRANSACTION COST IMPACT:\")\n",
    "print(f\"   ‚Ä¢ Average rebalancing magnitude: {avg_rebalancing:.3f}\")\n",
    "print(f\"   ‚Ä¢ Estimated daily transaction costs: ~{avg_rebalancing * 0.001:.4f} ({avg_rebalancing * 0.1:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38b1b2d",
   "metadata": {},
   "source": [
    "## üöÄ Conclusions and Future Extensions\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. **Custom RL Environment**: Built a realistic portfolio management environment with:\n",
    "   - Transaction costs\n",
    "   - Technical indicators\n",
    "   - Risk-adjusted rewards\n",
    "\n",
    "2. **Comprehensive Comparison**: Evaluated RL against traditional strategies:\n",
    "   - Buy and Hold\n",
    "   - Equal Weight Rebalancing\n",
    "   - Random allocation\n",
    "\n",
    "3. **Rigorous Testing**: Used out-of-sample testing to validate generalization\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "- **Sequential Decision Making**: RL naturally handles the temporal nature of portfolio management\n",
    "- **Risk-Return Optimization**: The agent learned to balance returns with risk and transaction costs\n",
    "- **Market Adaptation**: The agent can adapt to changing market conditions\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. **Market Assumptions**: Assumes historical patterns will continue\n",
    "2. **Limited Features**: Only used basic technical indicators\n",
    "3. **Transaction Costs**: Simplified cost model\n",
    "4. **Sample Period**: Limited to specific market conditions\n",
    "\n",
    "### üîÑ Potential Extensions\n",
    "\n",
    "1. **Enhanced Features**:\n",
    "   - Macro-economic indicators\n",
    "   - Sentiment analysis\n",
    "   - Alternative data sources\n",
    "\n",
    "2. **Advanced RL Algorithms**:\n",
    "   - SAC (Soft Actor-Critic)\n",
    "   - TD3 (Twin Delayed DDPG)\n",
    "   - Multi-agent approaches\n",
    "\n",
    "3. **Risk Management**:\n",
    "   - VaR constraints\n",
    "   - Drawdown limits\n",
    "   - Stress testing\n",
    "\n",
    "4. **Multi-Asset Classes**:\n",
    "   - Bonds, commodities, crypto\n",
    "   - Currency hedging\n",
    "   - International diversification\n",
    "\n",
    "### üí° Practical Considerations\n",
    "\n",
    "- **Model Validation**: Always test on out-of-sample data\n",
    "- **Risk Management**: Implement proper position sizing and stop-losses\n",
    "- **Market Regime Changes**: Monitor for structural breaks\n",
    "- **Transaction Costs**: Use realistic cost assumptions\n",
    "\n",
    "**Remember**: This is an educational demonstration. Real trading requires careful risk management, regulatory compliance, and thorough backtesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9795f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for future analysis\n",
    "results_summary = {\n",
    "    'experiment_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'assets': ASSETS,\n",
    "    'train_period': f\"{train_prices.index[0].date()} to {train_prices.index[-1].date()}\",\n",
    "    'test_period': f\"{test_prices.index[0].date()} to {test_prices.index[-1].date()}\",\n",
    "    'baseline_results': {k: v['stats'] for k, v in baseline_results.items()},\n",
    "    'rl_train_results': train_stats,\n",
    "    'rl_test_results': test_stats,\n",
    "    'model_parameters': {\n",
    "        'algorithm': 'PPO',\n",
    "        'total_timesteps': total_timesteps,\n",
    "        'transaction_cost': 0.001,\n",
    "        'initial_balance': 100000\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "import json\n",
    "with open('data/experiment_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2, default=str)\n",
    "\n",
    "print(\"üíæ Experiment results saved to 'data/experiment_results.json'\")\n",
    "print(\"\\nüéâ Laboratory completed successfully!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ REINFORCEMENT LEARNING FOR ASSET ALLOCATION - COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
