{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d770ff8",
   "metadata": {},
   "source": [
    "# Phi-3 Mini Tool Calling Laboratory\n",
    "\n",
    "## Introduction\n",
    "Welcome to the Phi-3 Tool Calling Laboratory! In this session, we will explore how to install and use Microsoft's Phi-3-mini locally with function calling capabilities.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this laboratory, you will be able to:\n",
    "1. Install and run Phi-3-mini locally using Ollama or Hugging Face\n",
    "2. Define custom tools/functions for the model to use\n",
    "3. Implement tool calling workflows\n",
    "4. Handle tool execution and response processing\n",
    "5. Create complex multi-tool scenarios\n",
    "6. Optimize performance for local deployment\n",
    "\n",
    "### Why Phi-3-mini?\n",
    "- **Lightweight**: Only ~3GB of RAM required\n",
    "- **Tool Calling**: Excellent support for function calling\n",
    "- **Educational**: Perfect for learning and experimentation\n",
    "- **Open Source**: No API keys or gated access required\n",
    "\n",
    "### Prerequisites\n",
    "- Python 3.9+\n",
    "- At least 8GB RAM (16GB+ recommended)\n",
    "- Optional: CUDA-compatible GPU for faster inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb563a97",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Import Libraries\n",
    "\n",
    "First, let's import all necessary libraries and check our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3074ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Custom tools imported successfully\n",
      "üêç Python version: 3.10.14 (main, Jun 10 2024, 11:31:15) [Clang 15.0.0 (clang-1500.3.9.4)]\n",
      "üìÅ Current working directory: /Volumes/Firestore/codeforge/rbs-nlp-rl-dm/llm-tool-calling\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "# Add tools directory to path\n",
    "sys.path.append('./tools')\n",
    "\n",
    "# Import our custom tools\n",
    "try:\n",
    "    from custom_tools import (\n",
    "        CalculatorTool, WeatherTool, FileOperationsTool, TextAnalysisTool,\n",
    "        TOOL_SCHEMAS, execute_tool\n",
    "    )\n",
    "    from ollama_client import OllamaClient, format_tool_call_prompt, extract_tool_call\n",
    "    print(\"‚úÖ Custom tools imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Could not import custom tools: {e}\")\n",
    "    print(\"üìù Note: Some imports may fail if dependencies aren't installed yet\")\n",
    "\n",
    "# Check Python version\n",
    "print(f\"üêç Python version: {sys.version}\")\n",
    "print(f\"üìÅ Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0368c1fa",
   "metadata": {},
   "source": [
    "## Step 2: Choose Your Installation Method\n",
    "\n",
    "We'll support two methods for running Phi-3-mini locally:\n",
    "1. **Ollama** (Recommended - easier setup)\n",
    "2. **Hugging Face Transformers** (More control)\n",
    "\n",
    "Let's check what's available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ac9cd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Environment Check:\n",
      "  Ollama available: ‚ùå\n",
      "  Transformers available: ‚úÖ\n",
      "\n",
      "üì¶ Using Hugging Face Transformers.\n"
     ]
    }
   ],
   "source": [
    "# Check if Ollama is available\n",
    "def check_ollama():\n",
    "    try:\n",
    "        import requests\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Check if transformers is available\n",
    "def check_transformers():\n",
    "    try:\n",
    "        import transformers\n",
    "        import torch\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "# Check available methods\n",
    "ollama_available = check_ollama()\n",
    "transformers_available = check_transformers()\n",
    "\n",
    "print(\"üîç Environment Check:\")\n",
    "print(f\"  Ollama available: {'‚úÖ' if ollama_available else '‚ùå'}\")\n",
    "print(f\"  Transformers available: {'‚úÖ' if transformers_available else '‚ùå'}\")\n",
    "\n",
    "if not ollama_available and not transformers_available:\n",
    "    print(\"\\n‚ö†Ô∏è Neither Ollama nor Transformers is available.\")\n",
    "    print(\"Please install one of them:\")\n",
    "    print(\"  Option 1: Install Ollama from https://ollama.ai/\")\n",
    "    print(\"  Option 2: pip install transformers torch\")\n",
    "elif ollama_available:\n",
    "    print(\"\\nüéâ Great! Ollama is running. We'll use that.\")\n",
    "    USE_OLLAMA = True\n",
    "else:\n",
    "    print(\"\\nüì¶ Using Hugging Face Transformers.\")\n",
    "    USE_OLLAMA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668699c",
   "metadata": {},
   "source": [
    "## Step 3: Initialize the Model\n",
    "\n",
    "Now let's initialize our chosen method for running Phi-3-mini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "157298e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing Hugging Face Transformers...\n",
      "‚úÖ Using Hugging Face API key\n",
      "üì• Loading tokenizer from microsoft/Phi-3-mini-4k-instruct...\n",
      "üéØ Phi-3-mini is specifically designed for tool calling and instruction following\n",
      "üì• Loading model from microsoft/Phi-3-mini-4k-instruct...\n",
      "‚è∞ This may take several minutes on first run...\n",
      "üñ•Ô∏è Using device: cpu\n",
      "üì• Loading model from microsoft/Phi-3-mini-4k-instruct...\n",
      "‚è∞ This may take several minutes on first run...\n",
      "üñ•Ô∏è Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb5782dae7b4a069e6672155ee3e154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 84\u001b[0m\n\u001b[1;32m     81\u001b[0m     test_input \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m test_input\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 84\u001b[0m     test_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtest_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use max_new_tokens instead of max_length\u001b[39;49;00m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Disable cache for initial test\u001b[39;49;00m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m test_response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(test_output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müß™ Test response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_response[:\u001b[38;5;241m100\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Volumes/Firestore/codeforge/rbs-nlp-rl-dm/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Firestore/codeforge/rbs-nlp-rl-dm/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2625\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2617\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2618\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2619\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2620\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2621\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2622\u001b[0m     )\n\u001b[1;32m   2624\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2625\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2632\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2635\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2636\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2637\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2638\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2639\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2640\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2641\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2642\u001b[0m     )\n",
      "File \u001b[0;32m/Volumes/Firestore/codeforge/rbs-nlp-rl-dm/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:3609\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3607\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3608\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3609\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3611\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3612\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3613\u001b[0m     outputs,\n\u001b[1;32m   3614\u001b[0m     model_kwargs,\n\u001b[1;32m   3615\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3616\u001b[0m )\n",
      "File \u001b[0;32m/Volumes/Firestore/codeforge/rbs-nlp-rl-dm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Firestore/codeforge/rbs-nlp-rl-dm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py:1243\u001b[0m, in \u001b[0;36mPhi3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1240\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1243\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1255\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1256\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/Volumes/Firestore/codeforge/rbs-nlp-rl-dm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Firestore/codeforge/rbs-nlp-rl-dm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py:1121\u001b[0m, in \u001b[0;36mPhi3Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1112\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1113\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         use_cache,\n\u001b[1;32m   1119\u001b[0m     )\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1121\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/Volumes/Firestore/codeforge/rbs-nlp-rl-dm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Firestore/codeforge/rbs-nlp-rl-dm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py:855\u001b[0m, in \u001b[0;36mPhi3DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    854\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 855\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    856\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_mlp_dropout(hidden_states)\n\u001b[1;32m    858\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m/Volumes/Firestore/codeforge/rbs-nlp-rl-dm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Firestore/codeforge/rbs-nlp-rl-dm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py:235\u001b[0m, in \u001b[0;36mPhi3MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    232\u001b[0m gate, up_states \u001b[38;5;241m=\u001b[39m up_states\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    233\u001b[0m up_states \u001b[38;5;241m=\u001b[39m up_states \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(gate)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mup_states\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Firestore/codeforge/rbs-nlp-rl-dm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Firestore/codeforge/rbs-nlp-rl-dm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Volumes/Firestore/codeforge/rbs-nlp-rl-dm/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if 'USE_OLLAMA' in locals() and USE_OLLAMA:\n",
    "    # Using Ollama\n",
    "    print(\"üöÄ Initializing Ollama client...\")\n",
    "    \n",
    "    try:\n",
    "        client = OllamaClient()\n",
    "        \n",
    "        # Check available models\n",
    "        models = client.list_models()\n",
    "        print(f\"üìã Available models: {models}\")\n",
    "        \n",
    "        # Test connection\n",
    "        test_response = client.generate(\"Hello! Please respond with 'Connection successful.'\")\n",
    "        print(f\"üß™ Test response: {test_response[:100]}...\")\n",
    "        \n",
    "        MODEL_INITIALIZED = True\n",
    "        print(\"‚úÖ Ollama client initialized successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing Ollama: {e}\")\n",
    "        MODEL_INITIALIZED = False\n",
    "\n",
    "else:\n",
    "    # Using Hugging Face Transformers\n",
    "    print(\"üöÄ Initializing Hugging Face Transformers...\")\n",
    "    \n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        import torch\n",
    "        import getpass\n",
    "        \n",
    "        # Check for Hugging Face token\n",
    "        hf_token = os.getenv('HUGGINGFACE_HUB_TOKEN')\n",
    "        \n",
    "        if not hf_token:\n",
    "            print(\"üîë Hugging Face API key required for gated models\")\n",
    "            print(\"You can get your token from: https://huggingface.co/settings/tokens\")\n",
    "            hf_token = getpass.getpass(\"Enter your Hugging Face API key: \")\n",
    "            \n",
    "            # Optionally save to environment for this session\n",
    "            os.environ['HUGGINGFACE_HUB_TOKEN'] = hf_token\n",
    "        \n",
    "        print(\"‚úÖ Using Hugging Face API key\")\n",
    "        \n",
    "        # Using Phi-3-mini: lightweight model with excellent tool calling capabilities\n",
    "        model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "        \n",
    "        print(f\"üì• Loading tokenizer from {model_name}...\")\n",
    "        print(\"üéØ Phi-3-mini is specifically designed for tool calling and instruction following\")\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name, \n",
    "            trust_remote_code=True,\n",
    "            use_fast=True\n",
    "        )\n",
    "        \n",
    "        print(f\"üì• Loading model from {model_name}...\")\n",
    "        print(\"‚è∞ This may take several minutes on first run...\")\n",
    "        \n",
    "        # Use appropriate device\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "        \n",
    "        # Load model with improved cache configuration\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\" if device == \"cuda\" else None,\n",
    "            trust_remote_code=True,\n",
    "            use_cache=True,\n",
    "            attn_implementation=\"eager\"  # Use eager attention to avoid cache issues\n",
    "        )\n",
    "        \n",
    "        # Add padding token if it doesn't exist\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Test the model with simpler generation parameters\n",
    "        test_input = tokenizer(\"Hello! Please respond with 'Model loaded successfully.'\", return_tensors=\"pt\")\n",
    "        if device == \"cuda\":\n",
    "            test_input = {k: v.to(device) for k, v in test_input.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_output = model.generate(\n",
    "                **test_input, \n",
    "                max_new_tokens=20,  # Use max_new_tokens instead of max_length\n",
    "                do_sample=True, \n",
    "                temperature=0.7,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=False  # Disable cache for initial test\n",
    "            )\n",
    "        \n",
    "        test_response = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
    "        print(f\"üß™ Test response: {test_response[:100]}...\")\n",
    "        \n",
    "        MODEL_INITIALIZED = True\n",
    "        print(\"‚úÖ Hugging Face model initialized successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing Phi-3-mini model: {e}\")\n",
    "        print(\"\\nüí° Troubleshooting tips:\")\n",
    "        print(\"   1. Ensure you have sufficient memory (Phi-3-mini needs ~3GB RAM)\")\n",
    "        print(\"   2. Check your internet connection for model downloads\")\n",
    "        print(\"   3. Try restarting the kernel if you see memory issues\")\n",
    "        print(\"   4. Make sure transformers>=4.36.0 is installed\")\n",
    "        print(\"   5. Try: pip install --upgrade transformers torch\")\n",
    "        MODEL_INITIALIZED = False\n",
    "\n",
    "if not MODEL_INITIALIZED:\n",
    "    print(\"\\n‚ö†Ô∏è Model initialization failed. Please check the setup.\")\n",
    "    print(\"You can still run the tool definition examples below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f7e447",
   "metadata": {},
   "source": [
    "## Step 4: Define Tools and Functions\n",
    "\n",
    "Let's explore the tools we've defined and understand how they work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae68fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display available tools\n",
    "print(\"üõ†Ô∏è Available Tools:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for tool_name, tool_schema in TOOL_SCHEMAS.items():\n",
    "    func_info = tool_schema['function']\n",
    "    print(f\"\\nüìã {func_info['name']}\")\n",
    "    print(f\"   Description: {func_info['description']}\")\n",
    "    print(f\"   Parameters: {list(func_info['parameters']['properties'].keys())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59e7158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our tools manually\n",
    "print(\"üß™ Testing Tools Manually:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Test calculator\n",
    "print(\"\\nüî¢ Calculator Test:\")\n",
    "result1 = execute_tool(\"calculator_add\", {\"a\": 15, \"b\": 27})\n",
    "print(f\"  15 + 27 = {result1}\")\n",
    "\n",
    "result2 = execute_tool(\"calculator_multiply\", {\"a\": 8, \"b\": 7})\n",
    "print(f\"  8 √ó 7 = {result2}\")\n",
    "\n",
    "# Test weather (mock)\n",
    "print(\"\\nüå§Ô∏è Weather Test:\")\n",
    "weather = execute_tool(\"get_weather\", {\"city\": \"Milan\", \"country\": \"IT\"})\n",
    "print(f\"  Weather in Milan: {json.dumps(weather, indent=2)}\")\n",
    "\n",
    "# Test text analysis\n",
    "print(\"\\nüìù Text Analysis Test:\")\n",
    "sample_text = \"Machine learning and artificial intelligence are transforming the way we work with data. Natural language processing enables computers to understand human language.\"\n",
    "analysis = execute_tool(\"analyze_text\", {\"text\": sample_text, \"max_keywords\": 5})\n",
    "print(f\"  Analysis results: {json.dumps(analysis, indent=2)}\")\n",
    "\n",
    "print(\"\\n‚úÖ All tools working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f73920",
   "metadata": {},
   "source": [
    "## Step 5: Basic Tool Calling with Phi-3\n",
    "\n",
    "Now let's implement tool calling with our Phi-3-mini model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2834c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_request(user_message: str, available_tools: List[Dict] = None) -> str:\n",
    "    \"\"\"Process a user request that might require tool calling\"\"\"\n",
    "    \n",
    "    if available_tools is None:\n",
    "        available_tools = list(TOOL_SCHEMAS.values())\n",
    "    \n",
    "    print(f\"üë§ User: {user_message}\")\n",
    "    print(\"ü§ñ Processing...\")\n",
    "    \n",
    "    if not MODEL_INITIALIZED:\n",
    "        return \"‚ùå Model not initialized. Cannot process request.\"\n",
    "    \n",
    "    try:\n",
    "        if USE_OLLAMA:\n",
    "            # Format prompt for tool calling\n",
    "            prompt = format_tool_call_prompt(user_message, available_tools)\n",
    "            \n",
    "            # Get model response\n",
    "            response = client.generate(prompt)\n",
    "            print(f\"üß† Model response: {response}\")\n",
    "            \n",
    "            # Check if the model wants to use a tool\n",
    "            tool_call = extract_tool_call(response)\n",
    "            \n",
    "            if tool_call:\n",
    "                print(f\"üõ†Ô∏è Tool call detected: {tool_call}\")\n",
    "                \n",
    "                # Execute the tool\n",
    "                tool_name = tool_call['name']\n",
    "                parameters = tool_call['parameters']\n",
    "                \n",
    "                try:\n",
    "                    tool_result = execute_tool(tool_name, parameters)\n",
    "                    print(f\"‚öôÔ∏è Tool result: {tool_result}\")\n",
    "                    \n",
    "                    # Generate final response incorporating tool result\n",
    "                    final_prompt = f\"\"\"The user asked: {user_message}\n",
    "                    \n",
    "You used the tool '{tool_name}' with parameters {parameters} and got this result: {tool_result}\n",
    "                    \n",
    "Please provide a helpful response to the user incorporating this information:\"\"\"\n",
    "                    \n",
    "                    final_response = client.generate(final_prompt)\n",
    "                    return final_response\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    return f\"‚ùå Error executing tool: {e}\"\n",
    "            else:\n",
    "                # No tool needed, return model response\n",
    "                return response\n",
    "                \n",
    "        else:\n",
    "            # Using Phi-3-mini with optimized prompt format\n",
    "            prompt = f\"\"\"<|system|>\n",
    "You are a helpful AI assistant with access to tools. When the user requests something that requires tools, respond with the appropriate tool call.\n",
    "\n",
    "Available tools:\n",
    "- calculator_add: Add two numbers\n",
    "- calculator_multiply: Multiply two numbers  \n",
    "- calculator_divide: Divide two numbers\n",
    "- get_weather: Get weather information for a city\n",
    "- analyze_text: Analyze text and extract keywords\n",
    "\n",
    "<|user|>\n",
    "{user_message}\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "            \n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=150,  # Use max_new_tokens instead of max_length\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    use_cache=False,  # Disable cache to avoid DynamicCache issues\n",
    "                    repetition_penalty=1.1\n",
    "                )\n",
    "            \n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            # Extract just the assistant's response\n",
    "            if \"<|assistant|>\" in response:\n",
    "                response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "            \n",
    "            return response\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error processing request: {e}\"\n",
    "\n",
    "print(\"üéØ Tool calling system ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6fcfc5",
   "metadata": {},
   "source": [
    "## Step 6: Interactive Examples\n",
    "\n",
    "Let's test our tool calling system with various requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99b4d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple calculation\n",
    "print(\"üìä Example 1: Calculation Request\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "response1 = process_user_request(\"What is 156 multiplied by 23?\")\n",
    "print(f\"ü§ñ Response: {response1}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710c3094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Weather query\n",
    "print(\"üå§Ô∏è Example 2: Weather Request\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "response2 = process_user_request(\"What's the weather like in Rome?\")\n",
    "print(f\"ü§ñ Response: {response2}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2da6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Text analysis\n",
    "print(\"üìù Example 3: Text Analysis Request\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "text_to_analyze = \"\"\"Artificial Intelligence is revolutionizing many industries. \n",
    "Machine learning algorithms can process vast amounts of data to identify patterns and make predictions. \n",
    "Deep learning, a subset of machine learning, uses neural networks to solve complex problems.\"\"\"\n",
    "\n",
    "response3 = process_user_request(f\"Please analyze this text and extract the main keywords: {text_to_analyze}\")\n",
    "print(f\"ü§ñ Response: {response3}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64338ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: No tool needed\n",
    "print(\"üí¨ Example 4: General Conversation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "response4 = process_user_request(\"What are the benefits of using local LLMs?\")\n",
    "print(f\"ü§ñ Response: {response4}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4739d263",
   "metadata": {},
   "source": [
    "## Step 7: Advanced Tool Calling Scenarios\n",
    "\n",
    "Let's create more complex scenarios involving multiple tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c0df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_process(user_message: str) -> str:\n",
    "    \"\"\"Handle requests that might need multiple tool calls\"\"\"\n",
    "    \n",
    "    print(f\"üéØ Multi-step processing: {user_message}\")\n",
    "    \n",
    "    # This is a simplified example - in a real system, you'd implement\n",
    "    # more sophisticated planning and execution\n",
    "    \n",
    "    if \"calculate\" in user_message.lower() and \"weather\" in user_message.lower():\n",
    "        # Example: \"Calculate the average temperature if Rome is 22¬∞C and Milan is 18¬∞C, then get weather for Naples\"\n",
    "        \n",
    "        # Step 1: Get weather for Naples\n",
    "        weather_result = execute_tool(\"get_weather\", {\"city\": \"Naples\", \"country\": \"IT\"})\n",
    "        \n",
    "        # Step 2: Calculate average (manual for this example)\n",
    "        avg_temp = execute_tool(\"calculator_add\", {\"a\": 22, \"b\": 18})\n",
    "        avg_temp = execute_tool(\"calculator_multiply\", {\"a\": avg_temp, \"b\": 0.5})\n",
    "        \n",
    "        return f\"Weather in Naples: {weather_result['condition']}, {weather_result['temperature']}¬∞C. Average of Rome and Milan: {avg_temp}¬∞C\"\n",
    "    \n",
    "    # Fall back to single-step processing\n",
    "    return process_user_request(user_message)\n",
    "\n",
    "# Test multi-step scenario\n",
    "print(\"üîó Multi-Step Example:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "multi_response = multi_step_process(\"Calculate the sum of 45 and 67, then analyze this text: 'Tool calling enables AI models to interact with external systems'\")\n",
    "print(f\"ü§ñ Multi-step response: {multi_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d71903",
   "metadata": {},
   "source": [
    "## Step 8: Performance Monitoring and Optimization\n",
    "\n",
    "Let's add some performance monitoring to our tool calling system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eeaf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timed_tool_execution(tool_name: str, parameters: Dict[str, Any]) -> tuple:\n",
    "    \"\"\"Execute a tool and measure execution time\"\"\"\n",
    "    start_time = time.time()\n",
    "    result = execute_tool(tool_name, parameters)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    return result, execution_time\n",
    "\n",
    "def benchmark_tools():\n",
    "    \"\"\"Benchmark all available tools\"\"\"\n",
    "    print(\"‚è±Ô∏è Benchmarking Tools:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    benchmarks = [\n",
    "        (\"calculator_add\", {\"a\": 100, \"b\": 200}),\n",
    "        (\"calculator_multiply\", {\"a\": 15, \"b\": 25}),\n",
    "        (\"get_weather\", {\"city\": \"Florence\", \"country\": \"IT\"}),\n",
    "        (\"analyze_text\", {\"text\": \"This is a sample text for analysis with various keywords and content.\", \"max_keywords\": 5})\n",
    "    ]\n",
    "    \n",
    "    for tool_name, params in benchmarks:\n",
    "        try:\n",
    "            result, exec_time = timed_tool_execution(tool_name, params)\n",
    "            print(f\"  {tool_name}: {exec_time*1000:.2f}ms\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {tool_name}: ERROR - {e}\")\n",
    "\n",
    "benchmark_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf9b68",
   "metadata": {},
   "source": [
    "## Step 9: Error Handling and Robustness\n",
    "\n",
    "Let's test how our system handles various error conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec417311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_error_handling():\n",
    "    \"\"\"Test various error conditions\"\"\"\n",
    "    print(\"üõ°Ô∏è Testing Error Handling:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Test 1: Invalid tool parameters\n",
    "    print(\"\\n1. Invalid calculator parameters:\")\n",
    "    try:\n",
    "        result = execute_tool(\"calculator_divide\", {\"a\": 10, \"b\": 0})\n",
    "        print(f\"   Result: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error (expected): {e}\")\n",
    "    \n",
    "    # Test 2: Missing required parameters\n",
    "    print(\"\\n2. Missing required parameters:\")\n",
    "    try:\n",
    "        result = execute_tool(\"calculator_add\", {\"a\": 5})  # Missing 'b'\n",
    "        print(f\"   Result: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error (expected): {e}\")\n",
    "    \n",
    "    # Test 3: Unknown tool\n",
    "    print(\"\\n3. Unknown tool:\")\n",
    "    try:\n",
    "        result = execute_tool(\"unknown_tool\", {})\n",
    "        print(f\"   Result: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error (expected): {e}\")\n",
    "    \n",
    "    # Test 4: Invalid text analysis\n",
    "    print(\"\\n4. Empty text analysis:\")\n",
    "    try:\n",
    "        result = execute_tool(\"analyze_text\", {\"text\": \"\"})\n",
    "        print(f\"   Result: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Error handling tests completed\")\n",
    "\n",
    "test_error_handling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b054ec5",
   "metadata": {},
   "source": [
    "## Step 10: Creating Custom Tools\n",
    "\n",
    "Let's demonstrate how to create and add new custom tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new custom tool\n",
    "class DataProcessingTool:\n",
    "    \"\"\"Advanced data processing operations\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_statistics(numbers: List[float]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate basic statistics for a list of numbers\"\"\"\n",
    "        if not numbers:\n",
    "            return {\"error\": \"Empty list provided\"}\n",
    "        \n",
    "        return {\n",
    "            \"count\": len(numbers),\n",
    "            \"sum\": sum(numbers),\n",
    "            \"mean\": sum(numbers) / len(numbers),\n",
    "            \"min\": min(numbers),\n",
    "            \"max\": max(numbers),\n",
    "            \"range\": max(numbers) - min(numbers)\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_sequence(start: int, end: int, step: int = 1) -> List[int]:\n",
    "        \"\"\"Generate a sequence of numbers\"\"\"\n",
    "        return list(range(start, end + 1, step))\n",
    "\n",
    "# Add schema for the new tool\n",
    "NEW_TOOL_SCHEMA = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"calculate_statistics\",\n",
    "        \"description\": \"Calculate basic statistics (mean, min, max, etc.) for a list of numbers\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"numbers\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"number\"},\n",
    "                    \"description\": \"List of numbers to analyze\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"numbers\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Test the new tool\n",
    "print(\"üîß Testing Custom Tool:\")\n",
    "test_numbers = [10, 20, 30, 40, 50, 15, 25, 35]\n",
    "stats = DataProcessingTool.calculate_statistics(test_numbers)\n",
    "print(f\"Numbers: {test_numbers}\")\n",
    "print(f\"Statistics: {json.dumps(stats, indent=2)}\")\n",
    "\n",
    "# Generate a sequence\n",
    "sequence = DataProcessingTool.generate_sequence(1, 10, 2)\n",
    "print(f\"\\nSequence (1 to 10, step 2): {sequence}\")\n",
    "\n",
    "print(\"\\n‚úÖ Custom tool works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97667cd0",
   "metadata": {},
   "source": [
    "## Step 11: Interactive Session\n",
    "\n",
    "Now let's create an interactive session where you can test tool calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bfb0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_session():\n",
    "    \"\"\"Run an interactive tool calling session\"\"\"\n",
    "    print(\"üéÆ Interactive Tool Calling Session\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Available commands:\")\n",
    "    print(\"  - Ask for calculations: 'What is 25 * 34?'\")\n",
    "    print(\"  - Ask for weather: 'What's the weather in Paris?'\")\n",
    "    print(\"  - Ask for text analysis: 'Analyze this text: [your text]'\")\n",
    "    print(\"  - Type 'quit' to exit\")\n",
    "    print(\"\\nNote: This is a demo - modify the cell below to test different queries\")\n",
    "    \n",
    "    # Example queries you can modify and run\n",
    "    example_queries = [\n",
    "        \"What is 144 divided by 12?\",\n",
    "        \"Analyze this text: Machine learning is transforming healthcare with predictive analytics and personalized medicine.\",\n",
    "        \"What's the weather like in Tokyo?\",\n",
    "        \"Calculate 15 plus 28\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüß™ Testing example queries:\")\n",
    "    for i, query in enumerate(example_queries, 1):\n",
    "        print(f\"\\n--- Example {i} ---\")\n",
    "        response = process_user_request(query)\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Response: {response}\")\n",
    "\n",
    "# Run the interactive session\n",
    "interactive_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ac507d",
   "metadata": {},
   "source": [
    "## Step 12: Best Practices and Optimization Tips\n",
    "\n",
    "Let's discuss some best practices for tool calling with local LLMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7592fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_best_practices():\n",
    "    \"\"\"Display best practices for tool calling\"\"\"\n",
    "    \n",
    "    practices = {\n",
    "        \"üéØ Tool Design\": [\n",
    "            \"Keep tools simple and focused on one task\",\n",
    "            \"Provide clear descriptions and parameter documentation\",\n",
    "            \"Include proper error handling and validation\",\n",
    "            \"Use meaningful parameter names and types\"\n",
    "        ],\n",
    "        \"‚ö° Performance\": [\n",
    "            \"Cache model responses when possible\",\n",
    "            \"Use quantized models for better speed/memory trade-off\",\n",
    "            \"Implement tool result caching for expensive operations\",\n",
    "            \"Consider async execution for I/O bound tools\"\n",
    "        ],\n",
    "        \"üõ°Ô∏è Security\": [\n",
    "            \"Validate all tool inputs thoroughly\",\n",
    "            \"Implement rate limiting for API calls\",\n",
    "            \"Sanitize file operations and paths\",\n",
    "            \"Use proper authentication for external services\"\n",
    "        ],\n",
    "        \"üêõ Debugging\": [\n",
    "            \"Log all tool calls and responses\",\n",
    "            \"Implement verbose mode for development\",\n",
    "            \"Test edge cases and error conditions\",\n",
    "            \"Monitor tool execution times\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"üìö Best Practices for Tool Calling:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for category, tips in practices.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for tip in tips:\n",
    "            print(f\"  ‚Ä¢ {tip}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    \n",
    "    # Performance metrics\n",
    "    if MODEL_INITIALIZED:\n",
    "        print(\"\\nüìä Current System Status:\")\n",
    "        print(f\"  Model: {'Ollama' if USE_OLLAMA else 'Hugging Face Transformers'}\")\n",
    "        print(f\"  Available tools: {len(TOOL_SCHEMAS)}\")\n",
    "        print(f\"  Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\" if 'torch' in locals() else \"  Device: Unknown\")\n",
    "\n",
    "display_best_practices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996bf145",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "üéâ **Congratulations!** You have successfully completed the Phi-3 Tool Calling Laboratory!\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **Local LLM Setup**: How to install and run Phi-3-mini locally using both Ollama and Hugging Face Transformers\n",
    "2. **Tool Definition**: How to create custom tools with proper schemas and parameter validation\n",
    "3. **Function Calling**: How to implement tool calling workflows with prompt engineering\n",
    "4. **Error Handling**: How to handle various error conditions gracefully\n",
    "5. **Performance Optimization**: Best practices for efficient tool calling\n",
    "6. **Advanced Scenarios**: Multi-step processing and complex tool interactions\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Expand Tool Library**: Create more specialized tools for your use case\n",
    "- **Integrate APIs**: Connect to real external services (weather, databases, etc.)\n",
    "- **Production Deployment**: Scale up for production use with proper caching and monitoring\n",
    "- **Fine-tuning**: Consider fine-tuning the model for better tool calling performance\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [Phi-3 Documentation](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)\n",
    "- [Ollama Documentation](https://ollama.ai/docs)\n",
    "- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)\n",
    "\n",
    "Happy coding with Phi-3 and tool calling! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
