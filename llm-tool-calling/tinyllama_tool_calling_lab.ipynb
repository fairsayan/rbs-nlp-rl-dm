{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d770ff8",
   "metadata": {},
   "source": [
    "# TinyLlama Tool Calling Lab\n",
    "\n",
    "## Introduction\n",
    "Welcome to a practical tool calling laboratory using TinyLlama-1.1B! This lab demonstrates how to implement function calling with a lightweight local model that uses minimal resources.\n",
    "\n",
    "### What You'll Learn\n",
    "1. Set up TinyLlama-1.1B for local inference\n",
    "2. Create and use custom tools (calculator, weather, text analysis)\n",
    "3. Implement pattern-based tool calling\n",
    "4. Handle tool execution and responses\n",
    "\n",
    "### Why TinyLlama?\n",
    "- **Lightweight**: Only ~1GB RAM required\n",
    "- **Fast**: Quick inference on CPU\n",
    "- **Local**: No API keys needed\n",
    "- **Educational**: Perfect for learning tool calling concepts\n",
    "\n",
    "### Prerequisites\n",
    "- Python 3.9+\n",
    "- ~2GB free disk space\n",
    "- Internet connection for model download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb563a97",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Import Libraries\n",
    "\n",
    "First, let's import all necessary libraries and check our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3074ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "# Add tools directory to path\n",
    "sys.path.append('./tools')\n",
    "\n",
    "# Import our custom tools\n",
    "try:\n",
    "    from custom_tools import (\n",
    "        CalculatorTool, WeatherTool, FileOperationsTool, TextAnalysisTool,\n",
    "        TOOL_SCHEMAS, execute_tool\n",
    "    )\n",
    "    from ollama_client import OllamaClient, format_tool_call_prompt, extract_tool_call\n",
    "    print(\"âœ… Custom tools imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Could not import custom tools: {e}\")\n",
    "    print(\"ðŸ“ Note: Some imports may fail if dependencies aren't installed yet\")\n",
    "\n",
    "# Check Python version\n",
    "print(f\"ðŸ Python version: {sys.version}\")\n",
    "print(f\"ðŸ“ Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0368c1fa",
   "metadata": {},
   "source": [
    "## Step 2: Environment Check\n",
    "\n",
    "Let's check if we have the required libraries and automatically set up TinyLlama-1.1B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac9cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if transformers is available\n",
    "def check_transformers():\n",
    "    try:\n",
    "        import transformers\n",
    "        import torch\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "# Simple environment check\n",
    "transformers_available = check_transformers()\n",
    "\n",
    "print(\"ðŸ” Environment Check:\")\n",
    "print(f\"  Transformers available: {'âœ…' if transformers_available else 'âŒ'}\")\n",
    "\n",
    "if transformers_available:\n",
    "    print(\"âœ… Ready to use TinyLlama with Transformers\")\n",
    "    USE_TRANSFORMERS = True\n",
    "else:\n",
    "    print(\"âŒ Please install: pip install transformers torch\")\n",
    "    USE_TRANSFORMERS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668699c",
   "metadata": {},
   "source": [
    "## Step 3: Load TinyLlama Model\n",
    "\n",
    "Now let's load TinyLlama-1.1B for tool calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157298e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_TRANSFORMERS:\n",
    "    print(\"âŒ Transformers not available. Please install required packages.\")\n",
    "    MODEL_INITIALIZED = False\n",
    "else:\n",
    "    print(\"ðŸš€ Loading TinyLlama-1.1B...\")\n",
    "    \n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        import torch\n",
    "        \n",
    "        # Check for optimal device (MPS on Mac M1/M2)\n",
    "        if torch.backends.mps.is_available():\n",
    "            device = \"mps\"  # Metal Performance Shaders for Mac\n",
    "            print(\"ðŸš€ Using Metal Performance Shaders (MPS) for Mac M2\")\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "            print(\"ðŸ–¥ï¸ Using CPU for inference\")\n",
    "        \n",
    "        # Load TinyLlama model\n",
    "        model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "        print(f\"ðŸ“¦ Loading {model_name}...\")\n",
    "        \n",
    "        # Optimize for Mac M2 - use float16 to reduce memory usage\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,  # Much more memory efficient\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=\"auto\"  # Automatic device placement\n",
    "        )\n",
    "        \n",
    "        # Move model to optimal device\n",
    "        model = model.to(device)\n",
    "        print(f\"ðŸ–¥ï¸ Model loaded on: {device}\")\n",
    "        print(\"ðŸ–¥ï¸ Using optimized settings for Mac M2\")\n",
    "        \n",
    "        # Set padding token\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Quick test\n",
    "        print(\"ðŸ§ª Testing model...\")\n",
    "        test_input = tokenizer(\"Hello!\", return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**test_input, max_new_tokens=3, do_sample=False)\n",
    "        \n",
    "        MODEL_INITIALIZED = True\n",
    "        print(\"âœ… TinyLlama loaded successfully!\")\n",
    "        print(f\"ðŸ’¾ Memory optimized for Mac M2 (using {device})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading model: {e}\")\n",
    "        MODEL_INITIALIZED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f7e447",
   "metadata": {},
   "source": [
    "## Step 4: Define Tools and Functions\n",
    "\n",
    "Let's explore the tools we've defined and understand how they work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae68fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display available tools\n",
    "print(\"ðŸ› ï¸ Available Tools:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for tool_name, tool_schema in TOOL_SCHEMAS.items():\n",
    "    func_info = tool_schema['function']\n",
    "    print(f\"\\nðŸ“‹ {func_info['name']}\")\n",
    "    print(f\"   Description: {func_info['description']}\")\n",
    "    print(f\"   Parameters: {list(func_info['parameters']['properties'].keys())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59e7158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our tools manually\n",
    "print(\"ðŸ§ª Testing Tools Manually:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Test calculator\n",
    "print(\"\\nðŸ”¢ Calculator Test:\")\n",
    "result1 = execute_tool(\"calculator_add\", {\"a\": 15, \"b\": 27})\n",
    "print(f\"  15 + 27 = {result1}\")\n",
    "\n",
    "result2 = execute_tool(\"calculator_multiply\", {\"a\": 8, \"b\": 7})\n",
    "print(f\"  8 Ã— 7 = {result2}\")\n",
    "\n",
    "# Test weather (mock)\n",
    "print(\"\\nðŸŒ¤ï¸ Weather Test:\")\n",
    "weather = execute_tool(\"get_weather\", {\"city\": \"Milan\", \"country\": \"IT\"})\n",
    "print(f\"  Weather in Milan: {json.dumps(weather, indent=2)}\")\n",
    "\n",
    "# Test text analysis\n",
    "print(\"\\nðŸ“ Text Analysis Test:\")\n",
    "sample_text = \"Machine learning and artificial intelligence are transforming the way we work with data. Natural language processing enables computers to understand human language.\"\n",
    "analysis = execute_tool(\"analyze_text\", {\"text\": sample_text, \"max_keywords\": 5})\n",
    "print(f\"  Analysis results: {json.dumps(analysis, indent=2)}\")\n",
    "\n",
    "print(\"\\nâœ… All tools working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f73920",
   "metadata": {},
   "source": [
    "## Step 5: Tool Calling Implementation\n",
    "\n",
    "Now let's implement tool calling with TinyLlama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2834c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_request(user_message: str) -> str:\n",
    "    \"\"\"Process user request with pattern-based tool calling\"\"\"\n",
    "    \n",
    "    print(f\"ðŸ‘¤ User: {user_message}\")\n",
    "    \n",
    "    if not MODEL_INITIALIZED:\n",
    "        return \"âŒ Model not initialized.\"\n",
    "    \n",
    "    # Simple pattern matching for tool calling\n",
    "    msg_lower = user_message.lower()\n",
    "    \n",
    "    # Check for calculations\n",
    "    calc_keywords = [\"multiply\", \"multiplied\", \"divide\", \"divided\", \"add\", \"plus\", \"calculate\", \"what is\"]\n",
    "    if any(keyword in msg_lower for keyword in calc_keywords):\n",
    "        return handle_calculation(user_message)\n",
    "    \n",
    "    # Check for weather\n",
    "    elif \"weather\" in msg_lower:\n",
    "        return handle_weather(user_message)\n",
    "    \n",
    "    # Check for text analysis\n",
    "    elif \"analyz\" in msg_lower and \"text\" in msg_lower:\n",
    "        return handle_text_analysis(user_message)\n",
    "    \n",
    "    # Default responses\n",
    "    else:\n",
    "        return \"I can help with calculations, weather queries, and text analysis. What would you like me to do?\"\n",
    "\n",
    "def handle_calculation(user_message: str) -> str:\n",
    "    \"\"\"Handle calculation requests\"\"\"\n",
    "    import re\n",
    "    numbers = re.findall(r'\\d+', user_message)\n",
    "    if len(numbers) >= 2:\n",
    "        a, b = int(numbers[0]), int(numbers[1])\n",
    "        if any(word in user_message.lower() for word in [\"multiply\", \"multiplied\", \"*\", \"times\"]):\n",
    "            result = execute_tool(\"calculator_multiply\", {\"a\": a, \"b\": b})\n",
    "            return f\"The result of {a} Ã— {b} is {result}\"\n",
    "        elif any(word in user_message.lower() for word in [\"divide\", \"divided\", \"/\"]):\n",
    "            result = execute_tool(\"calculator_divide\", {\"a\": a, \"b\": b})\n",
    "            return f\"The result of {a} Ã· {b} is {result}\"\n",
    "        else:\n",
    "            result = execute_tool(\"calculator_add\", {\"a\": a, \"b\": b})\n",
    "            return f\"The result of {a} + {b} is {result}\"\n",
    "    return \"Please provide two numbers for calculation.\"\n",
    "\n",
    "def handle_weather(user_message: str) -> str:\n",
    "    \"\"\"Handle weather requests\"\"\"\n",
    "    cities = [\"Rome\", \"Milan\", \"Naples\", \"Florence\"]\n",
    "    for city in cities:\n",
    "        if city.lower() in user_message.lower():\n",
    "            result = execute_tool(\"get_weather\", {\"city\": city, \"country\": \"IT\"})\n",
    "            return f\"Weather in {city}: {result['condition']}, {result['temperature']}Â°C\"\n",
    "    # Default to Rome\n",
    "    result = execute_tool(\"get_weather\", {\"city\": \"Rome\", \"country\": \"IT\"})\n",
    "    return f\"Weather in Rome: {result['condition']}, {result['temperature']}Â°C\"\n",
    "\n",
    "def handle_text_analysis(user_message: str) -> str:\n",
    "    \"\"\"Handle text analysis requests\"\"\"\n",
    "    text_start = user_message.find(\":\") + 1\n",
    "    if text_start > 0:\n",
    "        text_to_analyze = user_message[text_start:].strip()\n",
    "    else:\n",
    "        text_to_analyze = user_message\n",
    "    \n",
    "    if len(text_to_analyze) < 10:\n",
    "        return \"Please provide more text to analyze.\"\n",
    "    \n",
    "    result = execute_tool(\"analyze_text\", {\"text\": text_to_analyze, \"max_keywords\": 5})\n",
    "    return f\"Analysis complete! Found {result['word_count']} words and keywords: {', '.join(result['keywords'])}\"\n",
    "\n",
    "print(\"ðŸŽ¯ Tool calling system ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6fcfc5",
   "metadata": {},
   "source": [
    "## Step 6: Test Tool Calling\n",
    "\n",
    "Let's test our tool calling system with different types of requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99b4d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple calculation\n",
    "print(\"ðŸ“Š Example 1: Calculation Request\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "response1 = process_user_request(\"What is 156 multiplied by 23?\")\n",
    "print(f\"ðŸ¤– Response: {response1}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710c3094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Weather query\n",
    "print(\"ðŸŒ¤ï¸ Example 2: Weather Request\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "response2 = process_user_request(\"What's the weather like in Rome?\")\n",
    "print(f\"ðŸ¤– Response: {response2}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2da6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Text analysis\n",
    "print(\"ðŸ“ Example 3: Text Analysis Request\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "text_to_analyze = \"\"\"Artificial Intelligence is revolutionizing many industries. \n",
    "Machine learning algorithms can process vast amounts of data to identify patterns and make predictions. \n",
    "Deep learning, a subset of machine learning, uses neural networks to solve complex problems.\"\"\"\n",
    "\n",
    "response3 = process_user_request(f\"Please analyze this text and extract the main keywords: {text_to_analyze}\")\n",
    "print(f\"ðŸ¤– Response: {response3}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64338ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: No tool needed\n",
    "print(\"ðŸ’¬ Example 4: General Conversation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "response4 = process_user_request(\"What are the benefits of using local LLMs?\")\n",
    "print(f\"ðŸ¤– Response: {response4}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf9b68",
   "metadata": {},
   "source": [
    "## Step 8: Error Handling\n",
    "\n",
    "Let's test how our system handles error conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec417311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_error_handling():\n",
    "    \"\"\"Test various error conditions\"\"\"\n",
    "    print(\"ðŸ›¡ï¸ Testing Error Handling:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Test 1: Invalid tool parameters\n",
    "    print(\"\\n1. Invalid calculator parameters:\")\n",
    "    try:\n",
    "        result = execute_tool(\"calculator_divide\", {\"a\": 10, \"b\": 0})\n",
    "        print(f\"   Result: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error (expected): {e}\")\n",
    "    \n",
    "    # Test 2: Missing required parameters\n",
    "    print(\"\\n2. Missing required parameters:\")\n",
    "    try:\n",
    "        result = execute_tool(\"calculator_add\", {\"a\": 5})  # Missing 'b'\n",
    "        print(f\"   Result: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error (expected): {e}\")\n",
    "    \n",
    "    # Test 3: Unknown tool\n",
    "    print(\"\\n3. Unknown tool:\")\n",
    "    try:\n",
    "        result = execute_tool(\"unknown_tool\", {})\n",
    "        print(f\"   Result: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error (expected): {e}\")\n",
    "    \n",
    "    # Test 4: Invalid text analysis\n",
    "    print(\"\\n4. Empty text analysis:\")\n",
    "    try:\n",
    "        result = execute_tool(\"analyze_text\", {\"text\": \"\"})\n",
    "        print(f\"   Result: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error: {e}\")\n",
    "    \n",
    "    print(\"\\nâœ… Error handling tests completed\")\n",
    "\n",
    "test_error_handling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97667cd0",
   "metadata": {},
   "source": [
    "## Step 9: Interactive Testing\n",
    "\n",
    "Test more queries by modifying the examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bfb0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own queries here by modifying these examples:\n",
    "\n",
    "test_queries = [\n",
    "    \"What is 25 times 8?\",\n",
    "    \"What's the weather in Milan?\", \n",
    "    \"Analyze this text: TinyLlama is a small but powerful language model.\",\n",
    "    \"Calculate 100 plus 50\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ§ª Testing custom queries:\")\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n--- Test {i} ---\")\n",
    "    response = process_user_request(query)\n",
    "    print(f\"ðŸ¤– Response: {response}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Try changing the queries above to test different requests!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996bf145",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "ðŸŽ‰ **Congratulations!** You've successfully implemented tool calling with TinyLlama!\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **Model Setup**: Loaded TinyLlama-1.1B locally\n",
    "2. **Tool Integration**: Connected calculator, weather, and text analysis tools\n",
    "3. **Pattern Matching**: Implemented request classification without complex parsing\n",
    "4. **Tool Execution**: Handled tool calls and formatted responses\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Simple Patterns Work**: Basic keyword matching can effectively route requests\n",
    "- **Lightweight Models**: TinyLlama provides good functionality with minimal resources\n",
    "- **Direct Tool Calling**: You don't always need complex prompt engineering\n",
    "\n",
    "### Next Steps:\n",
    "- Try adding more tools (file operations, web search, etc.)\n",
    "- Experiment with different classification patterns\n",
    "- Test with more complex queries\n",
    "\n",
    "**Happy coding with TinyLlama!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
